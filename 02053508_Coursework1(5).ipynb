{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb_IWF6UQRGk"
   },
   "source": [
    "# Coursework 1 - Supervised learning\n",
    "\n",
    "**Replace CID in the file name with your CID**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTeLZnrzE0Wy"
   },
   "source": [
    "# Outline\n",
    "\n",
    "\n",
    "- [Task 1](#task-1): Regression <a name=\"index-task-1\"></a>\n",
    "  - [(1.1)](#task-11) Random Forest <a name=\"index-task-11\"></a>\n",
    "    - [(1.1.1)](#task-111) <a name=\"index-task-111\"></a>\n",
    "    - [(1.1.2)](#task-112) <a name=\"index-task-112\"></a>\n",
    "    - [(1.1.3)](#task-113) <a name=\"index-task-113\"></a>\n",
    "  - [(1.2)](#task-12) Multi-layer Perceptron <a name=\"index-task-12\"></a>\n",
    "    - [(1.2.1)](#task-121) <a name=\"index-task-121\"></a>\n",
    "    - [(1.2.2)](#task-122) <a name=\"index-task-122\"></a>\n",
    "    - [(1.2.3)](#task-123) <a name=\"index-task-123\"></a>\n",
    "- [Task 2](#task-2): Classification <a name=\"index-task-2\"></a>\n",
    "  - [(2.1)](#task-21) k-Nearest Neighbours <a name=\"index-task-21\"></a>\n",
    "    - [(2.1.1)](#task-211)  <a name=\"index-task-211\"></a>\n",
    "    - [(2.1.2)](#task-212) <a name=\"index-task-212\"></a>\n",
    "    - [(2.1.3)](#task-213) <a name=\"index-task-213\"></a>\n",
    "  - [(2.2)](#task-22) Logistic regression vs kernel logistic regression <a name=\"index-task-22\"></a>\n",
    "    - [(2.2.1)](#task-221) <a name=\"index-task-221\"></a>\n",
    "    - [(2.2.2)](#task-222) <a name=\"index-task-222\"></a>\n",
    "    - [(2.2.3)](#task-223) <a name=\"index-task-223\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4LmL6R9N1B-"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_QE32lOMff_"
   },
   "source": [
    "<a name=\"task-1\"></a>\n",
    "\n",
    "# (1) Task 1: Regression [(index)](#index-task-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLl66QsJMfzc"
   },
   "source": [
    "<a name=\"task-11\"></a>\n",
    "\n",
    "## (1.1) Random Forest [(index)](#index-task-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T47skSuUMf8M"
   },
   "source": [
    "<a name=\"task-111\"></a>\n",
    "\n",
    "### (1.1.1) [(index)](#index-task-111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a Decision Tree regression on the engineering data to see which feature determines the resultant electrical capacity of an electrode, and by how much.\n",
    "The summary of how I do this is:\n",
    "- Assign an index that indicates the factor that gives us the highest probability in determining the electrical capacity of an electrode.\n",
    "- Split the data using that chosen factor.\n",
    "- Repeat process on the condition of the chosen factor until we reach maximum depth or a pure node.\n",
    "\n",
    "I first read the training data and split it into the values of the features, `X_train`, and the values of the target, `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading sample and test data\n",
    "electrodes_sample = pd.read_csv('nanoelectrodes_capacitance_samples.csv')\n",
    "electrodes_test = pd.read_csv('nanoelectrodes_capacitance_test.csv')\n",
    "\n",
    "#Separating training and test data to its target and features\n",
    "y_train = electrodes_sample['Capacitance ($\\mu F / cm^2$)'].astype(int)\n",
    "X_train = electrodes_sample.iloc[:, list(range(6)) + list(range(7,13))]\n",
    "\n",
    "y_test = electrodes_test['Capacitance ($\\mu F / cm^2$)'].astype(int)\n",
    "X_test = electrodes_test.iloc[:, list(range(6)) + list(range(7,13))]\n",
    "\n",
    "# Helps clarify if the data in each feature is categorical or numerical i.e. discrete or continuous.\n",
    "columns_dict = {index: False for index in range(X_train.shape[1])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned index will be the gini index defined to be:\n",
    "$$ \n",
    "\\text{GI}(\\boldsymbol y) = 1 - \\sum_{i=1}^Q \\mathbb P (y = c_i)^2\n",
    "$$\n",
    "where $c_i$ is the i-th node out of $Q$ distinct nodes, so $\\mathbb P (y = c_i)$ reads the weight of the node $i$ in the current sample $\\boldsymbol y$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(y, sample_weights):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    Returns:\n",
    "        (float): the GINI-index for y.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize with zero for each distinct label.\n",
    "    label_weights = {yi: 0 for yi in set(y)}\n",
    "    for yi, wi in zip(y, sample_weights):\n",
    "        label_weights[yi] += wi\n",
    "\n",
    "    # The normalization constant\n",
    "    total_weight = sum(label_weights.values()) \n",
    "\n",
    "    sum_p_squared = 0\n",
    "    for weight in label_weights.values():\n",
    "        sum_p_squared += (weight / total_weight)**2  \n",
    "\n",
    "    # Return GINI-Index\n",
    "    return 1 - sum_p_squared "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of our training data is numeric, the following three functions defined are dedicatied on splitting the data based on a value that minimises the average weighted total of the gini indices, which is defined to be:\n",
    "\n",
    "$$GI(\\boldsymbol y; j, s) = p_l \\times GI(\\boldsymbol y_l) + p_r \\times GI(\\boldsymbol y_r)$$\n",
    "\n",
    "where $p_l$ and $p_r$ are, respectively, the cumulative weights of samples on the left and on the right. The variables $\\boldsymbol y, j$ and $s$ represent `y`, `split_column` and `value` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(X, y, sample_weights, column, value, categorical):\n",
    "    \"\"\"\n",
    "    Return the split of data whose column-th feature:\n",
    "      1. equals value, in case `column` is categorical, or\n",
    "      2. less than value, in case `column` is not categorical (i.e. numerical)\n",
    "\n",
    "    Arguments:\n",
    "        X: training features, of shape (N, p).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        column: the column of the feature for splitting.\n",
    "        value: splitting threshold  the samples\n",
    "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "    Returns:\n",
    "        tuple(np.array, np.array, np.array): tuple of the left split data (X_l, y_l, w_l).\n",
    "        tuple(np.array, np.array, np.array): tuple of the right split data (X_l, y_l, w_l)\n",
    "    \"\"\"\n",
    "\n",
    "    if categorical:\n",
    "        left_mask = (X[:, column] == value)\n",
    "    else:\n",
    "        left_mask = (X[:, column] < value)\n",
    "\n",
    "    # Using the binary masks `left_mask`, we split X, y, and sample_weights.\n",
    "    X_l, y_l, w_l = X[left_mask, :], y[left_mask], sample_weights[left_mask] \n",
    "    X_r, y_r, w_r = X[~left_mask, :], y[~left_mask], sample_weights[~left_mask] \n",
    "\n",
    "    return (X_l, y_l, w_l), (X_r, y_r, w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the best column based on the value of 'value' that minises the Gini Index\n",
    "def gini_split_value(X, y, sample_weights, column, categorical):\n",
    "    \"\"\"\n",
    "    Calculates the GINI-index based on `column` with the split that minimizes the GINI-index.\n",
    "    Arguments:\n",
    "        X: training features, of shape (N, p).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        column: the column of the feature for calculating. 0 <= column < D\n",
    "        categorical: boolean value indicating whether column is a categorical variable or numerical.\n",
    "    Returns:\n",
    "        (float, float): the resulted GINI-index and the corresponding value used in splitting.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_vals = np.unique(X[:, column])\n",
    "\n",
    "    assert len(unique_vals) > 1, f\"There must be more than one distinct feature value. Given: {unique_vals}.\"\n",
    "\n",
    "    gini_index_val, threshold = np.inf, None\n",
    "\n",
    "    # split the values of i-th feature and calculate the cost\n",
    "    for value in unique_vals:\n",
    "        (X_l, y_l, w_l), (X_r, y_r, w_r) = split_samples(X, y, sample_weights, column, value, categorical) \n",
    "\n",
    "        # if one of the two sides is empty, skip this split.\n",
    "        if len(y_l) == 0 or len(y_r) == 0:\n",
    "            continue\n",
    "\n",
    "        p_left = sum(w_l)/(sum(w_l) + sum(w_r))\n",
    "        p_right = 1 - p_left\n",
    "        new_cost = p_left * gini_index(y_l, w_l) + p_right * gini_index(y_r, w_r) \n",
    "        if new_cost < gini_index_val:\n",
    "              gini_index_val, threshold = new_cost, value\n",
    "\n",
    "    return gini_index_val, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_split(X, y, sample_weights, columns_dict):\n",
    "    \"\"\"\n",
    "    Chooses the best feature to split according to criterion.\n",
    "    Args:\n",
    "        X: training features, of shape (N, p).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "    Returns:\n",
    "        (int, float): the best feature index and value used in splitting.\n",
    "        If the feature index is None, then no valid split for the current Node.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize `split_column` to None, so if None returned this means there is no valid split at the current node.\n",
    "    min_gini_index = np.inf\n",
    "    split_column = None\n",
    "    split_val = np.nan\n",
    "\n",
    "    for column, categorical in columns_dict.items():\n",
    "        # skip column if samples are not seperable by that column.\n",
    "        if len(np.unique(X[:, column])) < 2:\n",
    "            continue\n",
    "        gini_index, current_split_val = gini_split_value(X, y, sample_weights, column, categorical)  \n",
    "\n",
    "\n",
    "        if gini_index < min_gini_index: \n",
    "            # Keep track with:\n",
    "\n",
    "            # 1. the current minimum gini-index value,\n",
    "            min_gini_index = gini_index \n",
    "\n",
    "            # 2. corresponding column,\n",
    "            split_column = column \n",
    "\n",
    "            # 3. corresponding split threshold.\n",
    "            split_val = current_split_val \n",
    "\n",
    "    return split_column, split_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `majority_vote` is added to ensure we cover all cases where the split doesn't/can't happen. It is done by simply taking the mode of the training target based on the weighting of each component $y_i$ in `y_train` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(y, sample_weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    Returns:\n",
    "        (int): the majority number\n",
    "    \"\"\"\n",
    "    majority_num = {yi: 0 for yi in set(y)}\n",
    "\n",
    "    for yi, wi in zip(y, sample_weights):\n",
    "        majority_num[yi] += wi\n",
    "\n",
    "    num_prediction = max(majority_num, key=majority_num.get) \n",
    "\n",
    "    return num_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `build_tree` completes the algorithm on determining the how each node will be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=10, min_samples_leaf=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: (np.array) training features, of shape (N, p).\n",
    "        y: (np.array) vector of training target values, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        feature_names (list): record the name of features in X in the original dataset.\n",
    "        depth (int): current depth for this node.\n",
    "    Returns:\n",
    "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "          1. 'feature_name': The column name of the split.\n",
    "          2. 'feature_index': The column index of the split.\n",
    "          3. 'value': The value used for the split.\n",
    "          4. 'categorical': indicator for categorical/numerical variables.\n",
    "          5. 'majority_num': For leaf nodes, this stores the dominant number. Otherwise, it is None.\n",
    "          6. 'left': The left sub-tree with the same structure.\n",
    "          7. 'right' The right sub-tree with the same structure.\n",
    "    \"\"\"\n",
    "    # include a clause for the cases where (i) no feature, (ii) all values are the same, (iii) depth exceed, or (iv) X is too small\n",
    "    if len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
    "        return {'majority_num': majority_vote(y, sample_weights)}\n",
    "\n",
    "    split_index, split_val = gini_split(X, y, sample_weights, columns_dict)  \n",
    "\n",
    "    # If no valid split at this node, use majority vote.\n",
    "    if split_index is None:\n",
    "        return {'majority_num': majority_vote(y, sample_weights)}\n",
    "\n",
    "    categorical = columns_dict[split_index]\n",
    "\n",
    "    # Split samples (X, y, sample_weights) given column, split-value, and categorical flag.\n",
    "    (X_l, y_l, w_l), (X_r, y_r, w_r) = split_samples(X, y, sample_weights, split_index, split_val, categorical) \n",
    "    return {\n",
    "        'feature_name': feature_names[split_index],\n",
    "        'feature_index': split_index,\n",
    "        'value': split_val,\n",
    "        'categorical': categorical,\n",
    "        'majority_num': None,\n",
    "        'left': build_tree(X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
    "        'right': build_tree(X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together leads us to `train` where `train` builds the entire tree based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y,  columns_dict, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Builds the decision tree according to the training data.\n",
    "    Args:\n",
    "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
    "        y: (pd.Series) vector of training target values, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    \"\"\"\n",
    "    if sample_weights is None:\n",
    "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    return build_tree(X, y, sample_weights, columns_dict, feature_names, depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_name': 'H9_Factor ($mL/g$)',\n",
       " 'feature_index': 7,\n",
       " 'value': 4.997162405405753,\n",
       " 'categorical': False,\n",
       " 'majority_num': None,\n",
       " 'left': {'feature_name': 'H9_Factor ($mL/g$)',\n",
       "  'feature_index': 7,\n",
       "  'value': 4.27729631685984,\n",
       "  'categorical': False,\n",
       "  'majority_num': None,\n",
       "  'left': {'majority_num': 220},\n",
       "  'right': {'majority_num': 203}},\n",
       " 'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "  'feature_index': 11,\n",
       "  'value': 19.76098089518472,\n",
       "  'categorical': False,\n",
       "  'majority_num': None,\n",
       "  'left': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "   'feature_index': 9,\n",
       "   'value': 83.80132235741195,\n",
       "   'categorical': False,\n",
       "   'majority_num': None,\n",
       "   'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "    'feature_index': 3,\n",
       "    'value': 9.36,\n",
       "    'categorical': False,\n",
       "    'majority_num': None,\n",
       "    'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "     'feature_index': 0,\n",
       "     'value': 410.2,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "      'feature_index': 2,\n",
       "      'value': 5.32,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'majority_num': 125},\n",
       "      'right': {'majority_num': 110}},\n",
       "     'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "      'feature_index': 0,\n",
       "      'value': 437.9,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'majority_num': 372},\n",
       "      'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "       'feature_index': 0,\n",
       "       'value': 444.5,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'majority_num': 156},\n",
       "       'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "        'feature_index': 0,\n",
       "        'value': 678.73,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 162},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 830.0,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 281},\n",
       "         'right': {'majority_num': 121}}}}}},\n",
       "    'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "     'feature_index': 11,\n",
       "     'value': 10.585985705192996,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "      'feature_index': 0,\n",
       "      'value': 178.6,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'majority_num': 160},\n",
       "      'right': {'majority_num': 183}},\n",
       "     'right': {'feature_name': 'Defect_Ratio',\n",
       "      'feature_index': 1,\n",
       "      'value': 1.47,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "       'feature_index': 2,\n",
       "       'value': 10.1,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "        'feature_index': 3,\n",
       "        'value': 16.0,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 298},\n",
       "        'right': {'majority_num': 113}},\n",
       "       'right': {'majority_num': 290}},\n",
       "      'right': {'majority_num': 332}}}},\n",
       "   'right': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "    'feature_index': 2,\n",
       "    'value': 5.74,\n",
       "    'categorical': False,\n",
       "    'majority_num': None,\n",
       "    'left': {'majority_num': 196},\n",
       "    'right': {'majority_num': 153}}},\n",
       "  'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "   'feature_index': 11,\n",
       "   'value': 49.34801312622436,\n",
       "   'categorical': False,\n",
       "   'majority_num': None,\n",
       "   'left': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "    'feature_index': 9,\n",
       "    'value': 92.09621265360884,\n",
       "    'categorical': False,\n",
       "    'majority_num': None,\n",
       "    'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "     'feature_index': 3,\n",
       "     'value': 27.56,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "      'feature_index': 9,\n",
       "      'value': 28.53525923092128,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "       'feature_index': 3,\n",
       "       'value': 4.76,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'majority_num': 234},\n",
       "       'right': {'feature_name': 'Defect_Ratio',\n",
       "        'feature_index': 1,\n",
       "        'value': 0.89,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "         'feature_index': 2,\n",
       "         'value': 5.483333333333333,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 280},\n",
       "         'right': {'majority_num': 231}},\n",
       "        'right': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "         'feature_index': 9,\n",
       "         'value': 25.313970573180548,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 140},\n",
       "         'right': {'majority_num': 132}}}},\n",
       "      'right': {'feature_name': 'Current_Density ($A/g$)',\n",
       "       'feature_index': 5,\n",
       "       'value': 0.7,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Defect_Ratio',\n",
       "        'feature_index': 1,\n",
       "        'value': 1.18,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "         'feature_index': 11,\n",
       "         'value': 26.800927789605986,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 241},\n",
       "         'right': {'majority_num': 212}},\n",
       "        'right': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "         'feature_index': 9,\n",
       "         'value': 47.01688154342425,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 195},\n",
       "         'right': {'majority_num': 209}}},\n",
       "       'right': {'feature_name': 'Doped_Sulfur_Percentage',\n",
       "        'feature_index': 4,\n",
       "        'value': 0.23,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "         'feature_index': 9,\n",
       "         'value': 61.55465624709353,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 238},\n",
       "         'right': {'majority_num': 201}},\n",
       "        'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "         'feature_index': 11,\n",
       "         'value': 26.652245257241628,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 200},\n",
       "         'right': {'majority_num': 150}}}}},\n",
       "     'right': {'feature_name': 'Doped_Flourine_Percentage',\n",
       "      'feature_index': 10,\n",
       "      'value': 51.61204335459556,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'H9_Factor ($mL/g$)',\n",
       "       'feature_index': 7,\n",
       "       'value': 15.70012340933149,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "        'feature_index': 9,\n",
       "        'value': 32.358594396179576,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 180},\n",
       "        'right': {'majority_num': 158}},\n",
       "       'right': {'majority_num': 149}},\n",
       "      'right': {'feature_name': 'Doped_Flourine_Percentage',\n",
       "       'feature_index': 10,\n",
       "       'value': 52.60961954081354,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'majority_num': 142},\n",
       "       'right': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "        'feature_index': 3,\n",
       "        'value': 28.41,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 167},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 131.3,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 184},\n",
       "         'right': {'majority_num': 160}}}}}},\n",
       "    'right': {'feature_name': 'Defect_Ratio',\n",
       "     'feature_index': 1,\n",
       "     'value': 1.21,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'majority_num': 297},\n",
       "     'right': {'feature_name': 'Defect_Ratio',\n",
       "      'feature_index': 1,\n",
       "      'value': 1.47,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'majority_num': 280},\n",
       "      'right': {'majority_num': 199}}}},\n",
       "   'right': {'feature_name': 'T5_Factor ($mL/g$)',\n",
       "    'feature_index': 8,\n",
       "    'value': 101.36501380476754,\n",
       "    'categorical': False,\n",
       "    'majority_num': None,\n",
       "    'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "     'feature_index': 3,\n",
       "     'value': 24.2,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'feature_name': 'Electrolyte_Concentration $[M]$',\n",
       "      'feature_index': 6,\n",
       "      'value': 3.0,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'Defect_Ratio',\n",
       "       'feature_index': 1,\n",
       "       'value': 1.32,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Doped_Flourine_Percentage',\n",
       "        'feature_index': 10,\n",
       "        'value': 38.80401246179729,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 438.7,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 120},\n",
       "         'right': {'majority_num': 253}},\n",
       "        'right': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "         'feature_index': 2,\n",
       "         'value': 8.2,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 175},\n",
       "         'right': {'majority_num': 103}}},\n",
       "       'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "        'feature_index': 11,\n",
       "        'value': 58.105727611937546,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 260},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 370.4,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 193},\n",
       "         'right': {'majority_num': 235}}}},\n",
       "      'right': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "       'feature_index': 9,\n",
       "       'value': 27.19338163037045,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "        'feature_index': 3,\n",
       "        'value': 6.8,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 1066.9,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 103},\n",
       "         'right': {'majority_num': 203}},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 280.0,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 155},\n",
       "         'right': {'majority_num': 253}}},\n",
       "       'right': {'feature_name': 'Doped_Carbon_Percentage',\n",
       "        'feature_index': 11,\n",
       "        'value': 50.035322447595725,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "         'feature_index': 2,\n",
       "         'value': 2.51,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 178},\n",
       "         'right': {'majority_num': 160}},\n",
       "        'right': {'feature_name': 'Defect_Ratio',\n",
       "         'feature_index': 1,\n",
       "         'value': 1.06,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 222},\n",
       "         'right': {'majority_num': 138}}}}},\n",
       "     'right': {'feature_name': 'Doped_Nitrogen_Percentage',\n",
       "      'feature_index': 2,\n",
       "      'value': 5.8500000000000005,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'Current_Density ($A/g$)',\n",
       "       'feature_index': 5,\n",
       "       'value': 10.0,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'majority_num': 172},\n",
       "       'right': {'majority_num': 130}},\n",
       "      'right': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "       'feature_index': 9,\n",
       "       'value': 63.479978084257645,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Doped_Flourine_Percentage',\n",
       "        'feature_index': 10,\n",
       "        'value': 31.39575868252513,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 93},\n",
       "        'right': {'majority_num': 124}},\n",
       "       'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "        'feature_index': 0,\n",
       "        'value': 471.05,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 161},\n",
       "        'right': {'majority_num': 140}}}}},\n",
       "    'right': {'feature_name': 'Doped_Zinc_Percentage',\n",
       "     'feature_index': 9,\n",
       "     'value': 26.637543069982065,\n",
       "     'categorical': False,\n",
       "     'majority_num': None,\n",
       "     'left': {'majority_num': 118},\n",
       "     'right': {'feature_name': 'Defect_Ratio',\n",
       "      'feature_index': 1,\n",
       "      'value': 1.18,\n",
       "      'categorical': False,\n",
       "      'majority_num': None,\n",
       "      'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "       'feature_index': 0,\n",
       "       'value': 108.6,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'majority_num': 67},\n",
       "       'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "        'feature_index': 0,\n",
       "        'value': 444.5,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 175.9,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 90},\n",
       "         'right': {'majority_num': 200}},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 1790.3,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 135},\n",
       "         'right': {'majority_num': 260}}}},\n",
       "      'right': {'feature_name': 'Doped_Oxygen_Percentage',\n",
       "       'feature_index': 3,\n",
       "       'value': 10.47,\n",
       "       'categorical': False,\n",
       "       'majority_num': None,\n",
       "       'left': {'feature_name': 'Electrolyte_Concentration $[M]$',\n",
       "        'feature_index': 6,\n",
       "        'value': 6.0,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 201},\n",
       "        'right': {'majority_num': 118}},\n",
       "       'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "        'feature_index': 0,\n",
       "        'value': 215.0,\n",
       "        'categorical': False,\n",
       "        'majority_num': None,\n",
       "        'left': {'majority_num': 126},\n",
       "        'right': {'feature_name': 'Surface_Area ($m^2/g$)',\n",
       "         'feature_index': 0,\n",
       "         'value': 1013.0,\n",
       "         'categorical': False,\n",
       "         'majority_num': None,\n",
       "         'left': {'majority_num': 36},\n",
       "         'right': {'majority_num': 95}}}}}}}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output of the Decision Tree \n",
    "tree = train(X_train, y_train, columns_dict)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions, namely `classify` and `predict`, will allow us to predict values for the resultant electrical capacity using the test data on the features, which is represented by `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x):\n",
    "    \"\"\"\n",
    "    Classify a single sample with the fitted decision tree.\n",
    "    Args:\n",
    "        x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "    Returns:\n",
    "        (int): predicted testing sample label.\n",
    "    \"\"\"\n",
    "    if tree['majority_num'] is not None:\n",
    "        return tree['majority_num']\n",
    "\n",
    "    elif tree['categorical']:\n",
    "        if x[tree['feature_index']] == tree['value']:\n",
    "            # go to left branch\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            # go to right branch\n",
    "            return classify(tree['right'], x)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if x[tree['feature_index']] < tree['value']: \n",
    "            # go to left branch\n",
    "            return classify(tree['left'], x)  \n",
    "        else:\n",
    "            # go to right branch\n",
    "            return classify(tree['right'], x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, X):\n",
    "    \"\"\"\n",
    "    Predict classification results for X.\n",
    "    Args:\n",
    "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
    "    Returns:\n",
    "        (np.array): predicted testing sample labels, of shape (N,).\n",
    "    \"\"\"\n",
    "    if len(X.shape) == 1:\n",
    "        return classify(tree, X)\n",
    "    else:\n",
    "        return np.array([classify(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the predicted values, `y_pred`, with the true data, `y_test` using the mean squared error:\n",
    "\n",
    "$$MSE = \\mathbb{E}[(y_{train} - y_{test})^2]$$ \n",
    "\n",
    "and the coefficient of determination:\n",
    "\n",
    "$$R^2 = 1 - \\frac{ \\sum^N_{i=1}{(y_{test_i} - \\bar{y}_{test})^2} }{ \\sum^N_{i=1}{(y_{test_i} - y_{pred_i})^2} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(yhat, y):\n",
    "    return np.mean((yhat - y)**2)\n",
    "\n",
    "def r_squared(y_pred, y):\n",
    "    s_1 = sum((y_test - np.mean(y_test))**2)\n",
    "    s_2 = sum((y_pred - y_test)**2)\n",
    "\n",
    "    return 1 - s_2/s_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 8145.322580645161\n",
      "R^2: -0.2326751742009936\n",
      "bias: -29.161290322580644\n",
      "var: 7294.941727367325\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(tree, X_test.to_numpy())\n",
    "print(f'MSE: {MSE(y_pred, y_test)}')\n",
    "print(f'R^2: {r_squared(y_pred, y_test)}')\n",
    "print(f'bias: {np.mean(y_pred - y_test)}')\n",
    "print(f'var: {MSE(y_pred, y_test) - np.mean(y_pred - y_test)**2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large $MSE$, negative $R^2$, relatively low bias implies that the variance in the residuals is very large, so the model constructed is perhaps overfitted and has very low predictive power or the model itself is not great for this data. This can be remedied by reducing the value of `max_depth` or simply choosing another model to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx4c9z5pMgDR"
   },
   "source": [
    "<a name=\"task-112\"></a>\n",
    "\n",
    "### (1.1.2) [(index)](#index-task-112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a Random Forest on the data, the procedure is similar to constructing a Decision Tree however we take an aggregate of many, but smaller, Decision Trees using randomly selected features in the dataset.\n",
    "\n",
    "Therefore the variables that need to be taken into consideration now are:\n",
    "- The number of features choses.\n",
    "- The number of Decision Trees built.\n",
    "\n",
    "Certain functions need to be modified in order to achieve this. \n",
    "\n",
    "The first being the function `gini_split_rf`, as it is similar to the function `gini_split`, however it picks the best feature and best condition to split the feature on by only using the randomly selected features chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_split_rf(n_features, X, y, sample_weights, columns_dict):\n",
    "    \"\"\"\n",
    "    Chooses the best feature to split according to criterion.\n",
    "    Args:\n",
    "        n_features: number of sampled features.\n",
    "        X: training features, of shape (N, p).\n",
    "        y: vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "    Returns:\n",
    "        (float, int, float): the minimized gini-index, the best feature index and value used in splitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # The added sampling step.\n",
    "    columns = np.random.choice(list(columns_dict.keys()), n_features, replace=False)\n",
    "    columns_dict = {c: columns_dict[c] for c in columns}\n",
    "\n",
    "    min_gini_index, split_column, split_val = np.inf, 0, 0\n",
    "\n",
    "    # Only scans through the sampled columns in `columns_dict`.\n",
    "    for column, categorical in columns_dict.items():\n",
    "        # skip column if samples are not seperable by that column.\n",
    "        if len(np.unique(X[:, column])) < 2:\n",
    "            continue\n",
    "\n",
    "        # searchs for the best splitting value for the given column.\n",
    "        gini_index, val = gini_split_value(X, y, sample_weights, column, categorical)  \n",
    "        if gini_index < min_gini_index:\n",
    "            min_gini_index, split_column, split_val = gini_index, column, val\n",
    "\n",
    "    return min_gini_index, split_column, split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the function `build_tree` needs to be modified to call `gini_split_rf` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_rf(n_features, X, y, sample_weights, columns_dict, feature_names, depth,  max_depth=6, min_samples_leaf=5):\n",
    "    \"\"\"Builds the decision tree according to the data.\n",
    "    Args:\n",
    "        X: (np.array) training features, of shape (N, p).\n",
    "        y: (np.array) vector of training labels, of shape (N,).\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        feature_names (list): record the name of features in X in the original dataset.\n",
    "        depth (int): current depth for this node.\n",
    "    Returns:\n",
    "        (dict): a dict denoting the decision tree (binary-tree). Each node has seven attributes:\n",
    "          1. 'feature_name': The column name of the split.\n",
    "          2. 'feature_index': The column index of the split.\n",
    "          3. 'value': The value used for the split.\n",
    "          4. 'categorical': indicator for categorical/numerical variables.\n",
    "          5. 'majority_num': For leaf nodes, this stores the dominant label. Otherwise, it is None.\n",
    "          6. 'left': The left sub-tree with the same structure.\n",
    "          7. 'right' The right sub-tree with the same structure.\n",
    "    \"\"\"\n",
    "    # includes a clause for the cases where (i) all lables are the same, (ii) depth exceed (iii) X is too small\n",
    "    if len(np.unique(y)) == 1 or depth>=max_depth or len(X)<=min_samples_leaf:\n",
    "        return {'majority_num': majority_vote(y, sample_weights)}\n",
    "\n",
    "    else:\n",
    "        GI, split_column, split_val = gini_split_rf(n_features, X, y, sample_weights, columns_dict)  \n",
    "\n",
    "        # If GI is infinity, it means that samples are not seperable by the sampled features.\n",
    "        if GI == np.inf:\n",
    "            return {'majority_num': majority_vote(y, sample_weights)}\n",
    "        categorical = columns_dict[split_column]\n",
    "        (X_l, y_l, w_l), (X_r, y_r, w_r) = split_samples(X, y, sample_weights, split_column, split_val, categorical) \n",
    "        return {\n",
    "            'feature_name': feature_names[split_column],\n",
    "            'feature_index': split_column,\n",
    "            'value': split_val,\n",
    "            'categorical': categorical,\n",
    "            'majority_num': None,\n",
    "            'left': build_tree_rf(n_features, X_l, y_l, w_l, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf),\n",
    "            'right': build_tree_rf(n_features, X_r, y_r, w_r, columns_dict, feature_names, depth + 1, max_depth, min_samples_leaf)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `train_rf` now builds multiple Decision Trees using the randomly selected subset of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(B, n_features, X, y,  columns_dict, sample_weights=None, max_depth = 6, min_samples_leaf = 5):\n",
    "    \"\"\"\n",
    "    Builds the decision tree according to the training data.\n",
    "    Args:\n",
    "        B: number of decision trees.\n",
    "        X: (pd.Dataframe) training features, of shape (N, p). Each X[i] is a training sample.\n",
    "        y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "        an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "        columns_dict: a dictionary mapping column indices to whether the column is categorical or numerical variable.\n",
    "        sample_weights: weights for each samples, of shape (N,).\n",
    "    \"\"\"\n",
    "    if sample_weights is None:\n",
    "        # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "        sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    N = X.shape[0]\n",
    "    training_indices = np.arange(N)\n",
    "    trees = []\n",
    "\n",
    "    for _ in range(B):\n",
    "        # Samples the training_indices (with replacement)\n",
    "        sample = np.random.choice(training_indices, N, replace=True) \n",
    "        X_sample = X[sample, :]\n",
    "        y_sample = y[sample]\n",
    "        w_sample = sample_weights[sample]\n",
    "        tree = build_tree_rf(n_features, X_sample, y_sample, w_sample,\n",
    "                             columns_dict, feature_names, depth=1, max_depth= max_depth, min_samples_leaf= min_samples_leaf)\n",
    "        trees.append(tree)\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `predict_rf` takes the Random Forest constructed and the data of the features and outputs the class with the highest probability.\n",
    "It does this by taking the average of all predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rf(rf, X):\n",
    "    \"\"\"\n",
    "    Predicts classification results for X.\n",
    "    Args:\n",
    "        rf: A trained random forest through train_rf function.\n",
    "        X: (pd.Dataframe) testing sample features, of shape (N, p).\n",
    "    Returns:\n",
    "        (np.array): predicted testing sample labels, of shape (N,).\n",
    "    \"\"\"\n",
    "\n",
    "    def aggregate(decisions):\n",
    "        \"\"\"\n",
    "        This function takes a list of predicted values produced by a list\n",
    "        of decision trees and returns the average of those values.\n",
    "        \"\"\"\n",
    "        return np.mean(decisions)\n",
    "    \n",
    "    if len(X.shape) == 1:\n",
    "        # if we have one sample\n",
    "        return aggregate([classify(tree, X) for tree in rf])\n",
    "    else:\n",
    "        # if we have multiple samples\n",
    "        return np.array([aggregate([classify(tree, x) for tree in rf]) for index, x in X.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs the 5-fold cross-validation on the model and outputs the average $MSE$ of the Random Forest for every value of $B$ inputted. The value chosen for the number of features is $\\approx \\frac{p}{3}$ where $p$ = the number of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(X_train, y_train, folds, B):\n",
    "  scores = []\n",
    "  p = X_train.shape[1]\n",
    "  # Iterating over each fold\n",
    "  for val_indexes in folds:\n",
    "    val_indexes = list(val_indexes)\n",
    "    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "    \n",
    "    # Establishing the training data for the cross-validation\n",
    "    X_train_i = X_train.iloc[train_indexes]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "    # Establishing the testing data for the cross-validation\n",
    "    X_val_i = X_train.iloc[val_indexes]\n",
    "    y_val_i = y_train[val_indexes] \n",
    "\n",
    "    #Training model and error in its prediction of y_val_i using MSE\n",
    "    rf = train_rf(B, round(p/3), X_train_i, y_train_i, columns_dict)\n",
    "    y_hat_i = predict_rf(rf, X_val_i)\n",
    "\n",
    "    score_i = MSE(y_hat_i, y_val_i)\n",
    "    scores.append(score_i)\n",
    "\n",
    "  # Returns the average MSE\n",
    "  return sum(scores) / len(scores) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the error between the model prediction and the actual values of y_test over a range of values of B\n",
    "def choose_best_B(X_train, y_train, folds, B_range):\n",
    "  B_scores = np.zeros((len(B_range),))\n",
    "\n",
    "  for i, B in enumerate(B_range):\n",
    "    B_scores[i] = cross_validation_score(X_train, y_train, folds, B)\n",
    "    print(f'MSE@k={B}: {B_scores[i]}')\n",
    "\n",
    "  best_k_index = np.argmin(B_scores)\n",
    "  return B_range[best_k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE@k=25: 6743.224358764479\n",
      "best_k: 25\n"
     ]
    }
   ],
   "source": [
    "# Splitting the training data in 5 to perform 5-fold cross-validation to find best value of B\n",
    "folds_indexes = np.array_split(np.arange(len(y_train)), 5)\n",
    "best_B = choose_best_B(X_train, y_train, folds_indexes, np.array([25]))\n",
    "\n",
    "print('best_k:', best_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the best value of $B$ is $25$ out of checking values from $1$ to $30$. I say this here as the computation ran for over an hour and yes I waited. At least it wasn't 2 hours.\n",
    "The $MSE$ at $B = 25$ is `6802.21407009009` which is a significant improvement over using the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008790298790450901"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking p/3 as an approximate for the number of features to choose\n",
    "p = round(X_train.shape[1]/3)\n",
    "\n",
    "# Predicting values of y_test and finding the error by using MSE\n",
    "rf = train_rf(25, p, X_train, y_train, columns_dict)\n",
    "y_pred = predict_rf(rf, X_test)\n",
    "r_squared(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the value of $R^2$ at $B= 25$ decreased in magnitude slightly, when comparing it to the Decision Tree, suggesting that the covariance between `y_pred` and `y_test` indeed decrease which is consistent with the findings of the $MSE$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5690.404257142857"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking an arbitrary fold to find a tree in the ensemble of B Decision Trees\n",
    "val_indexes = folds_indexes[1]\n",
    "train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "\n",
    "# Establishing the training data for the cross-validation\n",
    "X_train_i = X_train.iloc[train_indexes]\n",
    "y_train_i = y_train[train_indexes]\n",
    "\n",
    "# Establishing the testing data for the cross-validation\n",
    "X_val_i = X_train.iloc[val_indexes]\n",
    "y_val_i = y_train[val_indexes] \n",
    "\n",
    "# Training model and predicting values of \n",
    "rf_sub = train_rf(25, round(p/3), X_train_i, y_train_i, columns_dict)\n",
    "y_hat_i = predict_rf(rf, X_val_i)\n",
    "\n",
    "MSE(y_hat_i, y_val_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEwNJHHTMgNG"
   },
   "source": [
    "<a name=\"task-113\"></a>\n",
    "\n",
    "### (1.1.3) [(index)](#index-task-113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_depth_leaf(X_train, y_train, folds, B, max_depth, min_samples_leaf):\n",
    "  scores = []\n",
    "  p = X_train.shape[1]\n",
    "  # Iterating over each fold\n",
    "  for val_indexes in folds:\n",
    "    val_indexes = list(val_indexes)\n",
    "    train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "    \n",
    "    # Establishing the training data for the cross-validation\n",
    "    X_train_i = X_train.iloc[train_indexes]\n",
    "    y_train_i = y_train[train_indexes]\n",
    "\n",
    "    # Establishing the testing data for the cross-validation\n",
    "    X_val_i = X_train.iloc[val_indexes]\n",
    "    y_val_i = y_train[val_indexes] \n",
    "\n",
    "    #Training model and error in its prediction of y_val_i using MSE\n",
    "    rf = train_rf(B, round(p/3), X_train_i, y_train_i, columns_dict, max_depth= max_depth, min_samples_leaf= min_samples_leaf)\n",
    "    y_hat_i = predict_rf(rf, X_val_i)\n",
    "\n",
    "    score_i = MSE(y_hat_i, y_val_i)\n",
    "    scores.append(score_i)\n",
    "\n",
    "  # Returns the average MSE\n",
    "  return sum(scores) / len(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the error between the model prediction and the actual values of y_test over a range of values of B\n",
    "\n",
    "def best_depth_leaf(X_train, y_train, folds, max_depth_range, min_samples_leaf_range):\n",
    "  depth_leaf_scores = np.zeros((len(max_depth_range), len(min_samples_leaf_range)))\n",
    "\n",
    "  for i, max_depth in enumerate(max_depth_range):\n",
    "    for j, min_samples_leaf in enumerate(min_samples_leaf_range):\n",
    "        depth_leaf_scores[i, j] = cv_score_depth_leaf(X_train, y_train, folds, 25, max_depth = max_depth , min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "  min_array_index = np.argmin(depth_leaf_scores)\n",
    "  min_index_ij = np.unravel_index(min_array_index, depth_leaf_scores.shape)\n",
    "  return max_depth_range[min_index_ij[0]], min_samples_leaf_range[min_index_ij[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_depth_leaf(X_train, y_train, folds_indexes, np.arange(1,5), np.arange(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqN02H_YPwr0"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In8XkuW_MgVV"
   },
   "source": [
    "<a name=\"task-12\"></a>\n",
    "\n",
    "## (1.2) Multi-layer Perceptron [(index)](#index-task-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task-121\"></a>\n",
    "\n",
    "### (1.2.1) [(index)](#index-task-121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the construction of the network, I assume that each layer is fully connected with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(X, W, b):\n",
    "    \"\"\"Full-connected MLP layer.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.ndarray): K x h_in array of inputs, where K is the batch size and h_in if the input features dimension.\n",
    "        W (np.ndarray): h_out x h_in array for kernel matrix parametersm, where h_out is the output dimension.\n",
    "        b (np.ndarray): Length h_out 1-D array for bias parameters\n",
    "\n",
    "    Returns:\n",
    "        a (np.ndarray): K x h_out array of pre-activations\n",
    "    \"\"\"\n",
    "    a = np.vstack([W @ x + b for x in X]) \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the activation function for the model, which will be: \n",
    "$$ \\sigma(x) = tan^{-1}(x)ln(|x| + 1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(a):\n",
    "    \"\"\"ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "        a: K x h_out array of pre-activations\n",
    "\n",
    "    Returns:\n",
    "        h: K x h_out array of post-activations\n",
    "    \"\"\"\n",
    "    # computes post-activations\n",
    "    h = np.arctan(a) * np.log(np.abs(a) + 1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the activation function will be needed later for running 300 epochs of the MLP. Which will be:\n",
    "\n",
    "$$ \\sigma'(x) = \\frac{ln(|x|+1)}{x^2 + 1} + sgn(x)\\frac{tan^{-1}(x)}{|x| + 1}$$\n",
    "\n",
    "where, \n",
    "\n",
    "\\begin{equation*}\n",
    "sgn(x) = \\begin{cases}\n",
    "1 & \\quad x \\ge 0, \\\\\n",
    "-1 & \\quad x < 0.\n",
    "\\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_relu_activation(a):\n",
    "    \"\"\"Gradient of ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "        a: K x h_out array of pre-activations\n",
    "\n",
    "    Returns:\n",
    "        grad: K x h_out gradient array of post-activations\n",
    "    \"\"\"\n",
    "    # computes gradient\n",
    "    grad = np.log(np.abs(a) + 1)/(1 + a**2) + np.arctan(a)*np.sign(a)/(np.abs(a) + 1) \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `activation_table` will be used as a reference in the class `MLP` to know which activation function to use in each layer.\n",
    "\n",
    "The class `MLP` will be how the 3-layer perceptron will be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_table = {\n",
    "    \"relu\": relu_activation,\n",
    "    # Identity function.\n",
    "    \"identity\": lambda x: x\n",
    "}\n",
    "\n",
    "# A lookup table for gradient of activation functions by their names.\n",
    "grad_activation_table = {\n",
    "    \"relu\": grad_relu_activation,\n",
    "    # Identity function gradient.\n",
    "    \"identity\": lambda x: np.ones_like(x)\n",
    "}\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    This class represents a Multi-Layer Perceptron (MLP), that we are going\n",
    "    to use to encapsulate two components:\n",
    "        1. layers: the sequence of layers, where each layer is stored in\n",
    "            a dictionary in the format {\"W\": np.ndarray, \"b\": np.ndarray},\n",
    "            where \"W\" points to the weights array, and \"b\" points to\n",
    "            the bias vector.\n",
    "        2. rng: a pseudo random number generator (RNG) initialised to generate\n",
    "            the random weights in a reproducible manner between different\n",
    "            runtime sessions.\n",
    "    This class is also shipped with methods that perform essential operations\n",
    "    with a MLP, including:\n",
    "        - add_layers: which creates a new layer with specified dimensions.\n",
    "        - predict: applies the MLP forward pass to make predictions and produces\n",
    "            a computational graph for the forward pass that can be used to\n",
    "            compute gradients using backpropagation algorithm.\n",
    "        in addition to other light functions that return simple statistics about\n",
    "        the MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, seed=42):\n",
    "        self.layers = []\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def n_parameters(self):\n",
    "        \"\"\"Return the total number of parameters of weights and biases.\"\"\"\n",
    "        return sum(l[\"b\"].size + l[\"W\"].size for l in self.layers)\n",
    "\n",
    "    def n_layers(self):\n",
    "        \"\"\"Return current number of MLP layers.\"\"\"\n",
    "        return len(self.layers)\n",
    "\n",
    "    def layer_dim(self, index):\n",
    "        \"\"\"Retrieve the dimensions of the MLP layer at `index`.\"\"\"\n",
    "        return self.layers[index][\"W\"].shape\n",
    "\n",
    "    def add_layer(self, in_dim, out_dim, activation=\"identity\"):\n",
    "        \"\"\"Add fully connected layer to MLP.\n",
    "\n",
    "        Parameters:\n",
    "            in_dim (int): The output dimension of the layer.\n",
    "            out_dim (int): The input dimension of the layer.\n",
    "            activation (str): The activation function name.\n",
    "        \"\"\"\n",
    "        # check if input-dimension matches output-dimension of previous layer\n",
    "        if self.n_layers() > 0:\n",
    "            last_out_dim, _ = self.layer_dim(-1)\n",
    "            assert in_dim == last_out_dim, f\"Input-dimension {in_dim} does not match output-dimension {last_out_dim} of previous layer.\"\n",
    "\n",
    "        # the first layer, in our convention illustrated, does not apply activation on the input features X.\n",
    "        if self.n_layers() == 0:\n",
    "            assert activation == \"identity\", \"Should not apply activations on the input features X, use Identity function for the first layer.\"\n",
    "\n",
    "\n",
    "        # store each layer as a dictionary in the list, as shown in the\n",
    "        # attached diagram.\n",
    "        self.layers.append({\n",
    "            # only for debugging.\n",
    "            \"index\": len(self.layers),\n",
    "            # apply Glorot initialisation for weights.\n",
    "            \"W\": self.rng.normal(size=(out_dim, in_dim)) * np.sqrt(2. / (in_dim + out_dim)),\n",
    "            # initialise bias vector with zeros.\n",
    "            \"b\": np.zeros(out_dim), \n",
    "            # store the activation function (as string)\n",
    "            \"activation\": activation\n",
    "        })\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Apply the forward pass on the input X and produce prediction and the\n",
    "        forward computation graph.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray, List[Dict[str, np.ndarray]]): A tuple of the\n",
    "            predictions and the computation graph as a sequence of intermediate\n",
    "            values through the MLP, specifically each layer will have a corresponding\n",
    "            intermediate values {\"a\": np.ndarray, \"h\": np.ndarray}, as shown in the\n",
    "            attached diagram above.\n",
    "        \"\"\"\n",
    "        # We assume that we work with a batch of examples (ndim==2).\n",
    "        if X.ndim == 1:\n",
    "            # If one example passed, add a dummy dimension for the batch.\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        # store pre- and post-activations in list\n",
    "        forward_pass = [{\"index\": 0, \"a\": X, \"h\": X}]\n",
    "\n",
    "        # iterate through hidden layers\n",
    "        for k in range(1, len(self.layers)):\n",
    "            # compute pre-activations\n",
    "            a = dense(forward_pass[k - 1][\"h\"], self.layers[k - 1][\"W\"], self.layers[k - 1][\"b\"])\n",
    "            activation = activation_table[self.layers[k][\"activation\"]] \n",
    "            forward_pass.append({\"index\": k, \"a\" : a, \"h\" : activation(a)}) \n",
    "\n",
    "        y_hat = dense(forward_pass[-1][\"h\"], self.layers[-1][\"W\"], self.layers[-1][\"b\"]) \n",
    "        # predicted target is output of last layer\n",
    "        return y_hat, forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function will be the mean squared error defined as:\n",
    "$$ \\mathcal{l}(\\hat{\\boldsymbol{y}}, \\boldsymbol{y}) = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2, $$\n",
    "where $\\hat{\\boldsymbol{y}}$ is the vector of fitted values, $\\boldsymbol{y}$ is the vector of true values and $n$ is the dimension of the vector.\n",
    "\n",
    "In our case, the inputs will be one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Compute MSE-loss\n",
    "\n",
    "    Parameters:\n",
    "        y_true: ground-truth array, with shape (K, )\n",
    "        y_pred: predictions array, with shape (K, )\n",
    "\n",
    "    Returns:\n",
    "        loss (float): MSE-loss\n",
    "    \"\"\"\n",
    "    assert y_true.size == y_pred.size, \"Ground-truth and predictions have different dimensions.\"\n",
    "\n",
    "    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n",
    "    y_true = y_true.reshape(y_pred.shape)\n",
    "\n",
    "\n",
    "    return np.mean((y_true - y_pred)**2, keepdims=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need \n",
    "$$\\nabla_{\\hat{\\boldsymbol{y}}}\\mathcal{l}(\\hat{\\boldsymbol{y}}, \\boldsymbol{y}) = \\frac{2}{n}(\\hat{\\boldsymbol{y}} - \\boldsymbol{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse_loss(y_true, y_pred):\n",
    "    \"\"\"Compute gradient of MSE-loss\n",
    "\n",
    "    Parameters:\n",
    "        y_true: ground-truth values, shape: (K, ).\n",
    "        y_pred: prediction values, shape: (K, ).\n",
    "\n",
    "    Returns:\n",
    "        grad (np.ndarray): Gradient of MSE-loss, shape: (K, ).\n",
    "    \"\"\"\n",
    "    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n",
    "    y_true = y_true.reshape(y_pred.shape)\n",
    "\n",
    "    return 2.0 * (y_pred - y_true) / y_true.size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define the backpropagation algorithm to complete the stochastic gradient descent algorithm on the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(layers, forward_pass, delta_output):\n",
    "    \"\"\"\n",
    "    Apply the backpropagation algorithm to the MLP layers to compute the gradients starting from\n",
    "    the output layer to the input layer, and starting the chain rule from the\n",
    "    partial derivative of the loss function w.r.t the predictions $\\hat{y}$. The\n",
    "\n",
    "    Parameters:\n",
    "        layers (List[Dict[str, np.ndarray]]): The MLP sequence of layers, as shown in the diagrams.\n",
    "        forward_pass (List[Dict[str, np.ndarray]]): The forward pass intermediate values for\n",
    "            each layer, representing a computation graph.\n",
    "        delta_output (np.ndarray): the partial derivative of the loss function w.r.t the\n",
    "            predictions $\\hat{y}$, has the shape (K, 1), where K is the batch size.\n",
    "    Returns:\n",
    "        (List[Dict[str, np.ndarray]]): The computed gradient using a structure symmetric the layers, as shown\n",
    "            in the diagrams.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a list that will contain the gradients of all the layers.\n",
    "    delta = delta_output\n",
    "\n",
    "    assert len(layers) == len(forward_pass), \"Number of layers is expected to match the number of forward pass layers\"\n",
    "\n",
    "    # Iterate on layers backwardly, from output to input.\n",
    "    # Calculate gradients w.r.t. weights and biases of each level and store in list of dictionaries.\n",
    "    gradients = []\n",
    "    for layer, forward_computes in reversed(list(zip(layers, forward_pass))):\n",
    "        assert forward_computes[\"index\"] == layer[\"index\"], \"Mismatch in the index.\"\n",
    "\n",
    "        h = forward_computes[\"h\"]\n",
    "        assert delta.shape[0] == h.shape[0], \"Mismatch in the batch dimension.\"\n",
    "\n",
    "\n",
    "        gradients.append({\"W\" : delta.T @ h, \n",
    "                          \"b\" : delta.sum(axis=0)}) \n",
    "\n",
    "        # Update the delta for the next iteration\n",
    "        grad_activation_f = grad_activation_table[layer[\"activation\"]]\n",
    "        grad_activation = grad_activation_f(forward_computes[\"a\"])\n",
    "\n",
    "        # Calculate the delta for the backward layer.\n",
    "        delta = np.stack([np.diag(gi) @ layer[\"W\"].T @ di\n",
    "                           for (gi, di) in zip(grad_activation, delta)]) \n",
    "\n",
    "\n",
    "    # Return now ordered list matching the layers.\n",
    "    return list(reversed(gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to implement the algorithm for the SGD step for a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(X, y, mlp, learning_rate = 1e-3):\n",
    "    \"\"\"\n",
    "    Apply a stochastic gradient descent step using the sampled batch.\n",
    "    Parameters:\n",
    "        X (np.ndarray): The input features array batch, with dimension (K, D).\n",
    "        y (np.ndarray): The ground-truth of the batch, with dimension (K, 1).\n",
    "        learning_rate (float): The learning rate multiplier for the update steps in SGD.\n",
    "    Returns:\n",
    "        (List[Dict[str, np.ndarray]]): The updated layers after applying SGD.\n",
    "    \"\"\"\n",
    "    # Compute the forward pass.\n",
    "    y_hat, forward_pass = mlp.predict(X) \n",
    "\n",
    "    # Compute the partial derivative of the loss w.r.t. to predictions `y_hat`.\n",
    "    delta_output = grad_mse_loss(y, y_hat)\n",
    "\n",
    "    # Apply backpropagation algorithm to compute the gradients of the MLP parameters.\n",
    "    gradients = backpropagate(mlp.layers, forward_pass, delta_output) \n",
    "\n",
    "    # mlp.layers and gradients are symmetric, as shown in the figure.\n",
    "    updated_layers = []\n",
    "    for layer, grad in zip(mlp.layers, gradients):\n",
    "        W = layer[\"W\"] - learning_rate * grad[\"W\"]\n",
    "        b = layer[\"b\"] - learning_rate * grad[\"b\"]\n",
    "        updated_layers.append({\"W\": W, \"b\": b,\n",
    "                               # keep the activation function.\n",
    "                               \"activation\": layer[\"activation\"],\n",
    "                               # We use the index for asserts and debugging purposes only.\n",
    "                               \"index\": layer[\"index\"]})\n",
    "    return updated_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use the value of $R^2$ (along with the loss function) to determine performance of the model on the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y, y_hat):\n",
    "    \"\"\"R^2 score to assess regression performance.\"\"\"\n",
    "\n",
    "    # Adjustment to avoid subtraction between (K,) and (1, K) arrays.\n",
    "    y = y.reshape(y_hat.shape)\n",
    "    y_bar = y.mean()\n",
    "\n",
    "    ss_tot = ((y - y_bar)**2).sum()\n",
    "    ss_res = ((y - y_hat)**2).sum()\n",
    "    return 1 - (ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together everything to train the parameters of the MLP model using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X_train, y_train, X_test, y_test, mlp, learning_rate = 1e-3,\n",
    "        n_epochs=10, minibatchsize=1, seed=42):\n",
    "    \"\"\"\n",
    "    Run the Stochastic Gradient Descent (SGD) algorithm to optimise the parameters of MLP model to fit it on\n",
    "    the training data using MSE loss.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (np.ndarray): The training data features, with shape (|D_train|, D).\n",
    "        y_train (np.ndarray): The training data ground-truth, with shape (|D_train|, 1).\n",
    "        X_test (np.ndarray): The testing data features, with shape (|D_test|, D).\n",
    "        y_test (np.ndarray): The testing data ground-truth, with shape (|D_test|, 1).\n",
    "        mlp (MLP): The MLP object enacpsulating the MLP model.\n",
    "        learning_rate (float): The learning_rate multiplier used in updating the parameters at each iteration.\n",
    "        n_epochs (int): The number of training cycles that each covers the entire training examples.\n",
    "        minibatchsize (int): The batch size used in each SGD step.\n",
    "        seed (int): A seed for the RNG to ensure reproducibility across runtime sessions.\n",
    "    \"\"\"\n",
    "\n",
    "    # get random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # compute number of iterations per epoch\n",
    "    n_iterations = int(len(y_train) / minibatchsize)\n",
    "\n",
    "    # store losses\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "\n",
    "    epochs_bar = tqdm(range(n_epochs))\n",
    "    for i in epochs_bar:\n",
    "\n",
    "        # shuffle data\n",
    "        p = rng.permutation(len(y_train))\n",
    "        X_train_shuffled = X_train[p]\n",
    "        y_train_shuffled = y_train[p]\n",
    "\n",
    "        for j in range(n_iterations):\n",
    "            # get batch\n",
    "            X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
    "            y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
    "\n",
    "            # apply sgd step\n",
    "            updated_layers = sgd_step(X_batch, y_batch, mlp, learning_rate)\n",
    "\n",
    "            # update weights and biases of MLP\n",
    "            mlp.layers = updated_layers \n",
    "\n",
    "        # compute loss at the end of each epoch\n",
    "        y_hat_train, _ = mlp.predict(X_train)\n",
    "        losses_train.append(mse_loss(y_train, y_hat_train).squeeze())\n",
    "        y_hat_test, _ = mlp.predict(X_test)\n",
    "        losses_test.append(mse_loss(y_test, y_hat_test).squeeze())\n",
    "        epochs_bar.set_description(f'train_loss: {losses_train[-1]:.2f}, '\n",
    "                                   f'test_loss: {losses_test[-1]:.2f}, '\n",
    "                                   f'train_R^2: {r2_score(y_train, y_hat_train):.2f} '\n",
    "                                   f'test_R^2: {r2_score(y_test, y_hat_test):.2f} ')\n",
    "    return mlp, losses_train, losses_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best to standardise the training data before applying and training the model.\n",
    "This is because of the SGD step of subtracting the gradient of the loss function.\n",
    "\n",
    "If the training data is unstandardised, then teh value of each component of the loss function gradient may contain values of significantly different magnitudes, leading to a disproportionate result at certain axes of the updated parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X, X_train_=None):\n",
    "    \"\"\"Standardise features.\n",
    "\n",
    "    Parameters:\n",
    "        X (np.array): Feature matrix.\n",
    "        X_train_ (np.array): An optional feature matrix to compute the statistics\n",
    "            from before applying it to X. If None, just use X to compute the statistics.\n",
    "\n",
    "    Returns:\n",
    "        X_std (np.array): Standardised feature matrix\n",
    "    \"\"\"\n",
    "    if X_train_ is None:\n",
    "        X_train_ = X\n",
    "\n",
    "    mu = np.mean(X_train_, axis=0, keepdims=True) \n",
    "    sigma = np.std(X_train_, axis=0, keepdims=True) \n",
    "    X_std = (X - mu) / sigma \n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the training data and test it on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 4\n",
      "Number of trainable parameters: 5801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c22ab659e74a6c96bfeb124edafce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = MLP(seed=2)\n",
    "mlp.add_layer(X_train.shape[1], 50)\n",
    "mlp.add_layer(50, 50, \"relu\")\n",
    "mlp.add_layer(50, 50, \"relu\")\n",
    "mlp.add_layer(50, 1, \"relu\")\n",
    "print(\"Number of layers:\",mlp.n_layers())\n",
    "print(\"Number of trainable parameters:\",mlp.n_parameters())\n",
    "\n",
    "Z_train = standardise(X_train.to_numpy())\n",
    "Z_test = standardise(X_test.to_numpy())\n",
    "\n",
    "u_train = standardise(y_train.to_numpy())\n",
    "u_test = standardise(y_test.to_numpy())\n",
    "\n",
    "n_epochs = 300\n",
    "mlp, losses_train, losses_test = sgd(Z_train, u_train, Z_test, u_test,\n",
    "                                     mlp, learning_rate = 5e-5,\n",
    "                                     n_epochs=n_epochs,\n",
    "                                     minibatchsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the losses at each front and back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(np.arange(1,n_epochs+1),losses_train, label=\"Train\")\n",
    "ax.plot(np.arange(1,n_epochs+1),losses_test, label=\"Test\")\n",
    "ax.set(title=\"Losses vs Epochs\", xlabel = \"Epoch\", ylabel = \"MSE\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model becomes slightly more accurate for higher epochs, and you would also expect that the model would better fit the training data.\n",
    "\n",
    "It seems that the losses (mean squared error) converge to 0 for both lines which is what we want but it seems it would take a very long time for both lines, especially for the testing data, to reach 0.\n",
    "\n",
    "The training $R^2$ and the $R^2$ are not only similar but also very small. This is interesting but it perhaps might be to do with the size of the mini-batches. The greater the size, the closer it becomes to Batch Gradient Descent meaning that the value of $R^2$ for both types of data would be higher but with slower convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j1--gzeMgk9"
   },
   "source": [
    "<a name=\"task-122\"></a>\n",
    "\n",
    "### (1.2.2) [(index)](#index-task-122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply SDG with momentum we only need to make a few changes to certain functions, specifically `sdg_step` and `sdg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mm_step(X, y, mlp, learning_rate = 1e-3, momentum = 0.4):\n",
    "    \"\"\"\n",
    "    Apply a stochastic gradient descent with momentum step using the sampled batch.\n",
    "    Parameters:\n",
    "        X (np.ndarray): The input features array batch, with dimension (K, D).\n",
    "        y (np.ndarray): The ground-truth of the batch, with dimension (K, 1).\n",
    "        learning_rate (float): The learning rate multiplier for the update steps in SGD.\n",
    "    Returns:\n",
    "        (List[Dict[str, np.ndarray]]): The updated layers after applying SGD.\n",
    "    \"\"\"\n",
    "    # Compute the forward pass.\n",
    "    y_hat, forward_pass = mlp.predict(X) \n",
    "\n",
    "    # Compute the partial derivative of the loss w.r.t. to predictions `y_hat`.\n",
    "    delta_output = grad_mse_loss(y, y_hat)\n",
    "    \n",
    "    # Apply backpropagation algorithm to compute the gradients of the MLP parameters.\n",
    "    gradients = backpropagate(mlp.layers, forward_pass, delta_output) \n",
    "    \n",
    "    # Initialise the velocities for the algorithm.\n",
    "    v_t_dict = [{'W': np.zeros_like(layer['W']), 'b': np.zeros_like(layer['b'])} for layer in mlp.layers]\n",
    "   \n",
    "    # mlp.layers and gradients are symmetric, as shown in the figure.\n",
    "    updated_layers = []\n",
    "    for v_t, layer, grad in zip(v_t_dict, mlp.layers, gradients):\n",
    "        v_t['W'] = momentum * v_t['W'] + learning_rate * grad['W']\n",
    "        v_t['b'] = momentum * v_t['b'] + learning_rate * grad['b']\n",
    "        W = layer[\"W\"] - v_t['W'] \n",
    "        b = layer[\"b\"] - v_t['b'] \n",
    "        updated_layers.append({\"W\": W, \"b\": b,\n",
    "                               # keep the activation function.\n",
    "                               \"activation\": layer[\"activation\"],\n",
    "                               # We use the index for asserts and debugging purposes only.\n",
    "                               \"index\": layer[\"index\"]})\n",
    "    return updated_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mm(X_train, y_train, X_test, y_test, mlp, learning_rate = 1e-3, momentum_rate = 0.4,\n",
    "        n_epochs=10, minibatchsize=1, seed=42):\n",
    "    \"\"\"\n",
    "    Run the Stochastic Gradient Descent with Momentum (SGDM) algorithm to optimise the parameters of MLP model to fit it on\n",
    "    the training data using MSE loss.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (np.ndarray): The training data features, with shape (|D_train|, D).\n",
    "        y_train (np.ndarray): The training data ground-truth, with shape (|D_train|, 1).\n",
    "        X_test (np.ndarray): The testing data features, with shape (|D_test|, D).\n",
    "        y_test (np.ndarray): The testing data ground-truth, with shape (|D_test|, 1).\n",
    "        mlp (MLP): The MLP object enacpsulating the MLP model.\n",
    "        learning_rate (float): The learning_rate multiplier used in updating the parameters at each iteration.\n",
    "        momentum_rate (float): The momentum_rate multiplier used in updating the parameters at each iteration.\n",
    "        n_epochs (int): The number of training cycles that each covers the entire training examples.\n",
    "        minibatchsize (int): The batch size used in each SGD step.\n",
    "        seed (int): A seed for the RNG to ensure reproducibility across runtime sessions.\n",
    "    \"\"\"\n",
    "\n",
    "    # get random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # compute number of iterations per epoch\n",
    "    n_iterations = int(len(y_train) / minibatchsize)\n",
    "\n",
    "    # store losses\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "\n",
    "    epochs_bar = tqdm(range(n_epochs))\n",
    "    for i in epochs_bar:\n",
    "\n",
    "        # shuffle data\n",
    "        p = rng.permutation(len(y_train))\n",
    "        X_train_shuffled = X_train[p]\n",
    "        y_train_shuffled = y_train[p]\n",
    "\n",
    "        for j in range(n_iterations):\n",
    "            # get batch\n",
    "            X_batch = X_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
    "            y_batch = y_train_shuffled[j*minibatchsize : (j+1)*minibatchsize]\n",
    "\n",
    "            # apply sgdm step\n",
    "            updated_layers = sgd_mm_step(X_batch, y_batch, mlp, learning_rate, momentum_rate) \n",
    "\n",
    "            # update weights and biases of MLP\n",
    "            mlp.layers = updated_layers \n",
    "\n",
    "        # compute loss at the end of each epoch\n",
    "        y_hat_train, _ = mlp.predict(X_train)\n",
    "        losses_train.append(mse_loss(y_train, y_hat_train).squeeze())\n",
    "        y_hat_test, _ = mlp.predict(X_test)\n",
    "        losses_test.append(mse_loss(y_test, y_hat_test).squeeze())\n",
    "        epochs_bar.set_description(f'train_loss: {losses_train[-1]:.2f}, '\n",
    "                                   f'test_loss: {losses_test[-1]:.2f}, '\n",
    "                                   f'train_R^2: {r2_score(y_train, y_hat_train):.2f} '\n",
    "                                   f'test_R^2: {r2_score(y_test, y_hat_test):.2f} ')\n",
    "    return mlp, losses_train, losses_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 4\n",
      "Number of trainable parameters: 5801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5b523005d14a099f6d051e3d7a1072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = MLP(seed=2)\n",
    "mlp.add_layer(X_train.shape[1], 50)\n",
    "mlp.add_layer(50, 50, \"relu\")\n",
    "mlp.add_layer(50, 50, \"relu\")\n",
    "mlp.add_layer(50, 1, \"relu\")\n",
    "print(\"Number of layers:\",mlp.n_layers())\n",
    "print(\"Number of trainable parameters:\",mlp.n_parameters())\n",
    "\n",
    "Z_train = standardise(X_train.to_numpy())\n",
    "Z_test = standardise(X_test.to_numpy())\n",
    "\n",
    "u_train = standardise(y_train.to_numpy())\n",
    "u_test = standardise(y_test.to_numpy())\n",
    "\n",
    "n_epochs = 300\n",
    "mlp, losses_train_mom, losses_test_mom = sgd_mm(Z_train, u_train, Z_test, u_test,\n",
    "                                     mlp, learning_rate = 5e-5, momentum_rate=0.4,\n",
    "                                     n_epochs=n_epochs,\n",
    "                                     minibatchsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we plot the MSE for each model to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(losses_test == losses_test_mom)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "#ax.plot(np.arange(1,n_epochs+1),losses_test, label=\"Random Forest\")\n",
    "ax.plot(np.arange(1,n_epochs+1),losses_test, label=\"SGD\")\n",
    "ax.plot(np.arange(1,n_epochs+1),losses_test_mom, label=\"SGD w/ Momentum\")\n",
    "ax.set(xlabel = \"Epoch\", ylabel = \"MSE\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MccP5-sMgqU"
   },
   "source": [
    "<a name=\"task-123\"></a>\n",
    "\n",
    "### (1.2.3) [(index)](#index-task-123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the linear model to the quadratic basis of features we first need to create the matrix of interaction features which I will refer to as `X_prime_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_terms(X):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            X: an nxm np matrix with columns representing the features\n",
    "        Returns:\n",
    "            X': a matrix of interaction terms/features as columns\n",
    "    \"\"\"\n",
    "    X_t_prime = []\n",
    "    X_t_list = X.T.tolist()\n",
    "    \n",
    "    for row_i in X_t_list:\n",
    "        for row_j in X_t_list:\n",
    "            X_t_prime.append(np.array(row_i) * np.array(row_j))\n",
    "            \n",
    "    return np.array(X_t_prime).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prime_train = add_interaction_terms(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add functions that will determine the coefficients for the ridge regression linear model, $\\boldsymbol\\beta^{*}_{\\text{ridge}}$ which will be defined as:\n",
    "$$ \\boldsymbol\\beta^{*}_{\\text{ridge}} = (\\boldsymbol X^T\\boldsymbol X + \\lambda I)^{-1}\\boldsymbol X^T\\boldsymbol y \\, . $$\n",
    "where $\\lambda$ will the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_estimate(X, y, penalty):\n",
    "\n",
    "    # X: N x D matrix of training inputs\n",
    "    # y: N x 1 vector of training targets/observations\n",
    "    # returns: maximum likelihood parameters (D x 1)\n",
    "\n",
    "    N, D = X.shape\n",
    "    I = np.identity(D)\n",
    "    beta_ridge = np.linalg.solve(X.T @ X - penalty * I, X.T @ y)\n",
    "    return beta_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_estimate(X_test, beta):\n",
    "\n",
    "    # X_test: K x D matrix of test inputs\n",
    "    # beta: D x 1 vector of parameters\n",
    "    # returns: prediction of f(X_test); K x 1 vector\n",
    "\n",
    "    prediction = X_test @ beta \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ac5fcfb450>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "betas = []\n",
    "predictions = []\n",
    "penalties = [1e-3, 1, 1e+3]\n",
    "for penalty in penalties:\n",
    "    beta_ml = ridge_estimate(X_prime_train, y_train, penalty)\n",
    "    betas.append(beta_ml)\n",
    "    prediction = predict_with_estimate(X_prime_train, beta_ml)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "predictions = np.array(predictions)\n",
    "for beta, penalty in zip(betas, penalties):\n",
    "    plt.hist(beta, bins = 30, density = True, label = f'λ = {penalty}')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol\\beta^{*}_{\\text{ridge}}$  seems to Normally distributed for all three values of $\\lambda$ as we expected.\n",
    "\n",
    "It also seems that for each value of $\\lambda$, the distribution becomes less concentrated at $0$ as $\\lambda$ gets larger. \n",
    "\n",
    "Since we assume that $\\boldsymbol\\beta^{*}_{\\text{ridge}}$ is Normally distributed, this implies that larger values in $\\lambda$ lead to larger variances in the coefficients of the ridge linear regression.\n",
    "\n",
    "The variance of $\\boldsymbol\\beta^{*}_{\\text{ridge}}$ is $\\sigma^2(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I})^{-1}\\boldsymbol{X}^T\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I})^{-1}$ so for small values of $\\lambda$ we would expect:\n",
    "$$ Var(\\boldsymbol\\beta^{*}_{\\text{ridge}}) \\approx \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1} $$\n",
    "and for larger values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzPd5qZZPzpM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ1n70CNMguM"
   },
   "source": [
    "<a name=\"task-2\"></a>\n",
    "\n",
    "# (2) Task 2: Classification [(index)](#index-task-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snlMZuPqMgxd"
   },
   "source": [
    "<a name=\"task-21\"></a>\n",
    "\n",
    "## (2.1) k-Nearest Neighbours [(index)](#index-task-21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVwRifs5Mg0s"
   },
   "source": [
    "<a name=\"task-211\"></a>\n",
    "\n",
    "### (2.1.1) [(index)](#index-task-211)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a kNN classifier of the tumour type, optimise the model and assess its performance.\n",
    "\n",
    "First we read the data and split it for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_cancer_samples = pd.read_csv('brain_cancer_samples.csv')\n",
    "brain_cancer_test = pd.read_csv('brain_cancer_test.csv')\n",
    "\n",
    "class_index = brain_cancer_samples.columns.get_loc('Class')\n",
    "\n",
    "X_train = brain_cancer_samples.iloc[:, :class_index].to_numpy()\n",
    "y_train = brain_cancer_samples.iloc[:, class_index].to_numpy()\n",
    "\n",
    "X_test = brain_cancer_test.iloc[:, :class_index].to_numpy()\n",
    "y_test = brain_cancer_test.iloc[:, class_index].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the function `predict` which will (hopefully) predict the class of each patient in `X_test`. \n",
    "Before doing that, we construct the function `euclidian_distance` as a metric, and the `k_neighbours` as the function that carries out the majority of the kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p, q):\n",
    "    return np.sqrt(np.sum((p-q)**2, axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbours(X_train, X_test, k=5, return_distance=False):\n",
    "    n_neighbours = k\n",
    "    dist = []\n",
    "    neigh_ind = []\n",
    "\n",
    "    # computes distance from each point x_test in X_test to all points in X_train \n",
    "    point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test] \n",
    "\n",
    "    # determines which k training points are closest to each test point\n",
    "    for row in point_dist:\n",
    "        enum_neigh = enumerate(row)\n",
    "        sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n",
    "\n",
    "        ind_list = [tup[0] for tup in sorted_neigh]\n",
    "        dist_list = [tup[1] for tup in sorted_neigh]\n",
    "\n",
    "        dist.append(dist_list)\n",
    "        neigh_ind.append(ind_list)\n",
    "\n",
    "    # returns distances together with indices of k nearest neighbours\n",
    "    if return_distance:\n",
    "        return np.array(dist), np.array(neigh_ind)\n",
    "\n",
    "    return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, X_test, k=5):\n",
    "    # each of the k neighbours contributes equally to the classification of any data point in X_test\n",
    "    neighbours = k_neighbours(X_train, X_test, k=k)\n",
    "    # counts number of occurences of label with np.bincount and choose the label that has most with np.argmax \n",
    "    y_pred = np.array([np.argmax(np.bincount(y_train[neighbour])) for neighbour in neighbours])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see what the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_train, y_train, X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the 5-fold cross-validation on the kNN classifier.\n",
    "\n",
    "Using the `score` function as way of measuring loss, we take the average loss across each fold at a chosen $k$ value.\n",
    "\n",
    "The `score` function is defined to be $ \\frac{1}{n}\\sum_{i=1}^{n}{\\mathbb{1}(y_{pred_i} = y_{test_i})}$, where $y_{test}, y_{pred} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X_train, y_train, X_test, y_test, k=5):\n",
    "    y_pred = predict(X_train, y_train, X_test, k=k)\n",
    "    return float(sum(y_pred==y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(X_train, y_train, folds, k):\n",
    "    scores = []\n",
    "    for i in range(len(folds)):\n",
    "        val_indexes = folds[i]\n",
    "        train_indexes = list(set(range(y_train.shape[0])) - set(val_indexes))\n",
    "\n",
    "        X_train_i = X_train[train_indexes, :]\n",
    "        y_train_i = y_train[train_indexes]\n",
    "\n",
    "\n",
    "        X_val_i = X_train[val_indexes, :] \n",
    "        y_val_i = y_train[val_indexes] \n",
    "\n",
    "        score_i = score(X_train_i, y_train_i, X_val_i, y_val_i, k=k) \n",
    "        scores.append(score_i)\n",
    "\n",
    "    # Returns the average score\n",
    "    return sum(scores) / len(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_k(X_train, y_train, folds, k_range):\n",
    "    k_scores = np.zeros((len(k_range),))\n",
    "\n",
    "    for i, k in enumerate(k_range):\n",
    "        k_scores[i] = cross_validation_score(X_train, y_train, folds, k)\n",
    "        print(f'CV_ACC@k={k}: {k_scores[i]:.3f}')\n",
    "\n",
    "    best_k_index = np.argmax(k_scores)\n",
    "    return k_range[best_k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_ACC@k=1: 0.531\n",
      "CV_ACC@k=2: 0.524\n",
      "CV_ACC@k=3: 0.531\n",
      "CV_ACC@k=4: 0.614\n",
      "CV_ACC@k=5: 0.572\n",
      "CV_ACC@k=6: 0.593\n",
      "CV_ACC@k=7: 0.572\n",
      "CV_ACC@k=8: 0.586\n",
      "CV_ACC@k=9: 0.510\n",
      "CV_ACC@k=10: 0.538\n",
      "CV_ACC@k=11: 0.510\n",
      "CV_ACC@k=12: 0.517\n",
      "CV_ACC@k=13: 0.503\n",
      "CV_ACC@k=14: 0.476\n",
      "CV_ACC@k=15: 0.490\n",
      "CV_ACC@k=16: 0.483\n",
      "CV_ACC@k=17: 0.476\n",
      "CV_ACC@k=18: 0.490\n",
      "CV_ACC@k=19: 0.503\n",
      "CV_ACC@k=20: 0.497\n",
      "CV_ACC@k=21: 0.497\n",
      "CV_ACC@k=22: 0.490\n",
      "CV_ACC@k=23: 0.490\n",
      "CV_ACC@k=24: 0.517\n",
      "CV_ACC@k=25: 0.524\n",
      "CV_ACC@k=26: 0.503\n",
      "CV_ACC@k=27: 0.490\n",
      "CV_ACC@k=28: 0.510\n",
      "CV_ACC@k=29: 0.531\n",
      "CV_ACC@k=30: 0.524\n",
      "best_k: 4\n"
     ]
    }
   ],
   "source": [
    "folds_indexes = np.split(np.arange(len(y_train)), 5)\n",
    "\n",
    "best_k = choose_best_k(X_train, y_train, folds_indexes, np.arange(1, 31))\n",
    "\n",
    "print('best_k:', best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the best $k$ is 4 so for the next part let's first redefine `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_train, y_train, X_test, k = 4)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_classes = {0: 0, 1:0, 2:0}\n",
    "for types in y_pred:\n",
    "    freq_classes[types] = freq_classes.get(types, 0) + 1\n",
    "minority_class = min(freq_classes, key = freq_classes.get)\n",
    "minority_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to assess the predictions.\n",
    "Let's first create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TP': 11, 'FP': 14, 'TN': 22, 'FN': 13},\n",
       " {'TP': 20, 'FP': 12, 'TN': 13, 'FN': 15},\n",
       " {'TP': 2, 'FP': 1, 'TN': 31, 'FN': 26}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrices = [{} for  i in range(len(freq_classes.keys()))]\n",
    "\n",
    "for i in freq_classes:\n",
    "    confusion_matrices[i]['TP'] = confusion_matrices[i].get('TP', 0)\n",
    "    confusion_matrices[i]['FP'] = confusion_matrices[i].get('FP', 0)\n",
    "    confusion_matrices[i]['TN'] = confusion_matrices[i].get('TN', 0)\n",
    "    confusion_matrices[i]['FN'] = confusion_matrices[i].get('FN', 0)\n",
    "    for prediction, truth in zip(y_pred, y_test):\n",
    "        if prediction == truth and prediction == i:\n",
    "            confusion_matrices[i]['TP'] = confusion_matrices[i].get('TP') + 1\n",
    "            \n",
    "        elif prediction != truth and prediction == i:\n",
    "            confusion_matrices[i]['FP'] = confusion_matrices[i].get('FP') + 1\n",
    "            \n",
    "        elif prediction == truth and prediction != i:\n",
    "            confusion_matrices[i]['TN'] = confusion_matrices[i].get('TN') + 1\n",
    "        \n",
    "        else:\n",
    "            confusion_matrices[i]['FN'] = confusion_matrices[i].get('FN') + 1\n",
    "\n",
    "confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions used to define the six metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrices, average = 0):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            confusion_matrices (list): a list of confusion matrices which are stored as dictionaries\n",
    "            average (0 or 1): 0 for macro-average accuracy, 1 for micro-average accuracy\n",
    "            \n",
    "        Output:\n",
    "            accuracy (float): macro-average or micro-average accuracy depending on value of average.\n",
    "    \"\"\"\n",
    "    n = len(confusion_matrices)\n",
    "    \n",
    "    if average == 0: # for macro-average accuracy\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            M = confusion_matrices[i]\n",
    "            accuracy = (M['TP'] + M['FN']) / (sum(M.values()))\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        return np.mean(accuracy)\n",
    "    \n",
    "    if average == 1: # for micro-average accuracy\n",
    "        \n",
    "        tp_total = sum([confusion_matrices[i]['TP'] for i in range(n)])\n",
    "        tn_total = sum([confusion_matrices[i]['TN'] for i in range(n)])\n",
    "        fp_total = sum([confusion_matrices[i]['FP'] for i in range(n)])\n",
    "        fn_total = sum([confusion_matrices[i]['FN'] for i in range(n)])\n",
    "        \n",
    "        totals = [tp_total, tn_total, fp_total, fn_total]\n",
    "        \n",
    "        return (tp_total + tn_total) / sum(totals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(confusion_matrices, average = 0):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            confusion_matrices (list): a list of confusion matrices which are stored as dictionaries\n",
    "            average (0 or 1): 0 for macro-average precision, 1 for micro-average precision\n",
    "            \n",
    "        Output:\n",
    "            accuracy (float): macro-average or micro-average precision depending on value of average.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(confusion_matrices)\n",
    "    \n",
    "    if average == 0: # for macro-average precision\n",
    "        \n",
    "        precisions = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            M = confusion_matrices[i]\n",
    "            precision = M['TP'] / (M['TP'] + M['FP'])\n",
    "            precisions.append(precision)\n",
    "            \n",
    "        return np.mean(precisions)\n",
    "    \n",
    "    if average == 1: # for micro-average precision\n",
    "        \n",
    "        tp_total = sum([confusion_matrices[i]['TP'] for i in range(n)])\n",
    "        fp_total = sum([confusion_matrices[i]['FP'] for i in range(n)])\n",
    "        \n",
    "        return tp_total / (tp_total + fp_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weighted(confusion_matrices, a_or_p):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            confusion_matrices (list): a list of confusion matrices which are stored as dictionaries\n",
    "            a_or_p ('a' or 'p'): picks the type of average: accuracy or precision\n",
    "        Output:\n",
    "            class-weighted average precision or accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(confusion_matrices)\n",
    "    sum_of_classes = sum([sum(confusion_matrices[i].values()) for i in range(n)])\n",
    "    class_weights = {i: sum(confusion_matrices[i].values()) / sum_of_classes for i in range(n)}\n",
    "    \n",
    "    if a_or_p == 'a': # for class-weighted average accuracy\n",
    "        \n",
    "        accuracies = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            M = confusion_matrices[i]\n",
    "            accuracy = (M['TP'] + M['FN']) / (sum(M.values()))\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        weighted_accuracies = [accuracy * class_weight for accuracy, class_weight in zip(accuracies, class_weights.values())]\n",
    "    \n",
    "        return np.mean(weighted_accuracies) \n",
    "\n",
    "    if a_or_p == 'p': # for class-weighted average accuracy\n",
    "        \n",
    "        precisions = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            M = confusion_matrices[i]\n",
    "            precision = M['TP'] / (M['TP'] + M['FP'])\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        weighted_precisions = [precision * class_weight for precision, class_weight in zip(precisions, class_weights.values())]\n",
    "        \n",
    "        return np.mean(weighted_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average accuracy:  0.4666666666666667\n",
      "Micro-average accuracy:  0.55\n",
      "Macro-average precision:  0.5772222222222222\n",
      "Micro-average precision:  0.55\n",
      "Class-weighted average accuracy:  0.16111111111111112\n",
      "Class-weighted average precision:  0.1924074074074074\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average accuracy: ', accuracy(confusion_matrices, 0))\n",
    "print('Micro-average accuracy: ', accuracy(confusion_matrices, 1))\n",
    "print('Macro-average precision: ', precision(confusion_matrices, 0))\n",
    "print('Micro-average precision: ', precision(confusion_matrices, 1))\n",
    "print('Class-weighted average accuracy: ', class_weighted(confusion_matrices, 'a'))\n",
    "print('Class-weighted average precision: ', class_weighted(confusion_matrices, 'p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The micro-average accuracy is greater than the macro-average accuracy however the macro-average precision is slightly greater than the micro-average precision and the class-weigthed averages is both low, implying that the model does not predict the minority class well.\n",
    "\n",
    "You can get that from inspection of the confusion matrix. The minority class is class 2. The amount of false negatives were high and the amount of true positive were low.\n",
    "Additionally, the amount of true negatives were high and amount of false positives were low.\n",
    "\n",
    "This implies that class 2 had too little significance when training the model, and the model opts for the other classes too often when it should be class 2 that should be opted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrF3U23EMg3k"
   },
   "source": [
    "<a name=\"task-212\"></a>\n",
    "\n",
    "### (2.1.2) [(index)](#index-task-212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model opts for the other classes too often, the idea is to weight the classes such that the minority class has more of a priority.\n",
    "\n",
    "I attempt to do this by weighting the classes by $1/p_i$ where $p_i$ is the proportion of class $i$ in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_weighted_neighbours(X_train, y_train, X_test, k=5, weight_parameter = 1, return_distance=False):\n",
    "    n_neighbours = k\n",
    "    dist = []\n",
    "    neigh_ind = []\n",
    "\n",
    "    # computes distance from each point x_test in X_test to all points in X_train \n",
    "    #point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test] \n",
    "    # freq_classes is dictionary that contains the frequency of each class in the training dataset X_train\n",
    "    freq_classes = {0:0, 1:0, 2:0}\n",
    "    \n",
    "    for i, x_test in enumerate(X_test):\n",
    "        if y_train[i] == 0:\n",
    "            freq_classes[0] = freq_classes.get(0, 0) + 1\n",
    "        elif y_train[i] == 1:\n",
    "            freq_classes[1] = freq_classes.get(1, 0) + 1\n",
    "        else:\n",
    "            freq_classes[2] = freq_classes.get(2, 0) + 1\n",
    "            \n",
    "    ordered_classes = sorted(freq_classes.items(), key = lambda x: x[1])\n",
    "    \n",
    "    for i, (key, value) in enumerate(ordered_classes):\n",
    "        freq_classes[key] = 10**(len(ordered_classes) - i)\n",
    "    \n",
    "    point_dist = [euclidian_distance(x_test, X_train) / (freq_classes[y_train[i]]) for i, x_test in enumerate(X_test)] \n",
    "    \n",
    "    # determines which k training points are closest to each test point\n",
    "    for row in point_dist:\n",
    "        enum_neigh = enumerate(row)\n",
    "        sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n",
    "\n",
    "        ind_list = [tup[0] for tup in sorted_neigh]\n",
    "        dist_list = [tup[1] for tup in sorted_neigh]\n",
    "\n",
    "        dist.append(dist_list)\n",
    "        neigh_ind.append(ind_list)\n",
    "\n",
    "    # returns distances together with indices of k nearest neighbours\n",
    "    if return_distance:\n",
    "        return np.array(dist), np.array(neigh_ind)\n",
    "\n",
    "    return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted(X_train, y_train, X_test, k=5):\n",
    "    # each of the k neighbours contributes equally to the classification of any data point in X_test\n",
    "    neighbours = k_weighted_neighbours(X_train, y_train, X_test, k=k)\n",
    "    # counts number of occurences of label with np.bincount and choose the label that has most with np.argmax \n",
    "    y_pred = np.array([np.argmax(np.bincount(y_train[neighbour])) for neighbour in neighbours]) \n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I show the result below to explain what happens a bit later. When determining the performance of the weighted kNN using the same six metrics used before, I get the exact same values for each metric and the result below shows why.\n",
    "\n",
    "The attempted implementation of the weighting had no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n",
      "[0 1 0 1 1 1 0 1 1 1 1 1 1 1 2 1 0 1 1 0 0 0 1 0 0 0 0 2 0 0 2 0 1 0 0 1 0\n",
      " 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_weighted = predict_weighted(X_train, y_train, X_test, k=4)\n",
    "print(y_pred_weighted == y_pred)\n",
    "print(y_pred_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TP': 11, 'FP': 14, 'TN': 22, 'FN': 13},\n",
       " {'TP': 20, 'FP': 12, 'TN': 13, 'FN': 15},\n",
       " {'TP': 2, 'FP': 1, 'TN': 31, 'FN': 26}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrices = [{} for  i in range(len(freq_classes.keys()))]\n",
    "\n",
    "for i in freq_classes:\n",
    "    confusion_matrices[i]['TP'] = confusion_matrices[i].get('TP', 0)\n",
    "    confusion_matrices[i]['FP'] = confusion_matrices[i].get('FP', 0)\n",
    "    confusion_matrices[i]['TN'] = confusion_matrices[i].get('TN', 0)\n",
    "    confusion_matrices[i]['FN'] = confusion_matrices[i].get('FN', 0)\n",
    "    for prediction, truth in zip(y_pred_weighted, y_test):\n",
    "        if prediction == truth and prediction == i:\n",
    "            confusion_matrices[i]['TP'] = confusion_matrices[i].get('TP') + 1\n",
    "            \n",
    "        elif prediction != truth and prediction == i:\n",
    "            confusion_matrices[i]['FP'] = confusion_matrices[i].get('FP') + 1\n",
    "            \n",
    "        elif prediction == truth and prediction != i:\n",
    "            confusion_matrices[i]['TN'] = confusion_matrices[i].get('TN') + 1\n",
    "        \n",
    "        else:\n",
    "            confusion_matrices[i]['FN'] = confusion_matrices[i].get('FN') + 1\n",
    "\n",
    "confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average accuracy:  0.4666666666666667\n",
      "Micro-average accuracy:  0.55\n",
      "Macro-average precision:  0.5772222222222222\n",
      "Micro-average precision:  0.55\n",
      "Class-weighted average accuracy:  0.16111111111111112\n",
      "Class-weighted average precision:  0.1924074074074074\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average accuracy: ', accuracy(confusion_matrices, 0))\n",
    "print('Micro-average accuracy: ', accuracy(confusion_matrices, 1))\n",
    "print('Macro-average precision: ', precision(confusion_matrices, 0))\n",
    "print('Micro-average precision: ', precision(confusion_matrices, 1))\n",
    "print('Class-weighted average accuracy: ', class_weighted(confusion_matrices, 'a'))\n",
    "print('Class-weighted average precision: ', class_weighted(confusion_matrices, 'p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIUs15tlMg9c"
   },
   "source": [
    "<a name=\"task-213\"></a>\n",
    "\n",
    "### (2.1.3) [(index)](#index-task-213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6Zz2bcMP6Wk"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0K6bbpHPUYE"
   },
   "source": [
    "<a name=\"task-22\"></a>\n",
    "\n",
    "## (2.2) Logistic regression vs kernel logistic regression [(index)](#index-task-22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pVdpUHZP-oE"
   },
   "source": [
    "<a name=\"task-221\"></a>\n",
    "\n",
    "### (2.2.1) [(index)](#index-task-221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to extract the training and testing data that only belong to either class 1 or class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_train = []\n",
    "y_log_train = []\n",
    "\n",
    "for i, x, y in zip(range(len(y_train)), X_train, y_train):\n",
    "    if y == 1 or y == 2:\n",
    "        X_log_train.append(x)\n",
    "        y_log_train.append(y)\n",
    "       \n",
    "# the training data for logistic regression\n",
    "X_log_train = np.array(X_log_train)\n",
    "y_log_train = np.array(y_log_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_test = []\n",
    "y_log_test = []\n",
    "\n",
    "for i, x, y in zip(range(len(y_test)), X_test, y_test):\n",
    "    if y == 1 or y == 2:\n",
    "        X_log_test.append(x)\n",
    "        y_log_test.append(y)\n",
    "    else:\n",
    "        continue\n",
    "       \n",
    "# the training data for logistic regression\n",
    "X_log_test = np.array(X_log_test)\n",
    "y_log_test = np.array(y_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function\n",
    "def logistic(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function where the input is the dot product of x with beta plus a constant beta0\n",
    "def predict_logistic(X, beta0, beta):\n",
    "    y_logistic = logistic(X @ beta + beta0) \n",
    "    return y_logistic.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to initialise beta and beta0 both as vector of zeroes.\n",
    "def initialise(size):\n",
    "    beta = np.zeros((size, 1))\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function will be used to complete each step in the iteration of the gradient descent algorithm.\n",
    "def propagate(X, y, beta0, beta, penalty = 2.5e-3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: Data of shape (N, p+1)\n",
    "    y: True label vector of size N\n",
    "    beta: Parameter vector, a numpy array of size p+1\n",
    "\n",
    "    Returns:\n",
    "    mean_loss: Mean sample loss for the negative log-likelihood\n",
    "    dbeta: Gradient of the mean sample loss with respect to beta\n",
    "\n",
    "    \"\"\"\n",
    "    y_log = predict_logistic(X, beta0, beta)\n",
    "\n",
    "    # Mean sample loss function\n",
    "    k = np.mean(y_log)\n",
    "    A = np.nan_to_num(np.log(y_log), nan = k, posinf = k, neginf = k)\n",
    "    B = np.nan_to_num(np.log(1 - y_log), nan = k, posinf = k, neginf = k)\n",
    "    \n",
    "    mean_loss = - np.mean(y * A + (1-y) * B) + (penalty/2) * np.sum(beta**2)\n",
    "\n",
    "    # Derivatives\n",
    "    dbeta0 = np.mean(y - y_log).reshape(-1, 1)\n",
    "    dbeta = np.mean(X.T * (y_log - y) + penalty * beta, axis=1).reshape(-1, 1) \n",
    "    \n",
    "    mean_loss = np.squeeze(mean_loss)\n",
    "  \n",
    "    # Store gradients in a dictionary\n",
    "    grads = {'dbeta0': dbeta0, 'dbeta': dbeta}\n",
    "  \n",
    "    return grads, mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the gradient descent algorithm to calculate the 'best' beta\n",
    "def optimise(X, y, beta0, beta, num_iterations=1000, learning_rate=0.1, penalty = 2.5e-3 , print_loss=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: Data of shape (N, p+1)\n",
    "    y: True label vector of size N\n",
    "    beta: Parameter vector, a numpy array of size p+1\n",
    "    num_iterations: Number of iterations\n",
    "    learning_rate: Step size in updating procedure\n",
    "    print_loss: 'True' to print the mean loss every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    params: Dictionary containing the parameter vector beta\n",
    "    grads: Dictionary containing the gradient\n",
    "    mean_loss_history: List of all the mean loss values computed during the optimisation (can be used to plot the learning curve)\n",
    "\n",
    "    \"\"\"\n",
    "    mean_loss_history = []\n",
    "    \n",
    "    for i in range(int(num_iterations)):\n",
    "\n",
    "        # Calculating the loss and gradients \n",
    "        grads, mean_loss = propagate(X, y, beta0, beta, penalty)  \n",
    "      \n",
    "        # Retrieving derivatives from grads\n",
    "        dbeta0 = grads['dbeta0']\n",
    "        dbeta = grads['dbeta']\n",
    "      \n",
    "        # Updating procedure\n",
    "        beta0 = beta0 - learning_rate * dbeta0\n",
    "        beta = beta - learning_rate * dbeta  \n",
    "        \n",
    "        # Records the loss values\n",
    "        if i % 100 == 0:\n",
    "            mean_loss_history.append(mean_loss)\n",
    "      \n",
    "        # Printing the loss every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print ('Mean loss after iteration %i: %f' %(i, mean_loss))\n",
    "  \n",
    "    # Saving parameters and gradients in dictionary\n",
    "    params = {'beta0': beta0, 'beta': beta}\n",
    "    grads = {'dbeta0': dbeta0, 'dbeta': dbeta}\n",
    "  \n",
    "    return params, grads, mean_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to fit model\n",
    "def predict(X_test, beta0, beta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_test: Test set of shape (N_test, p+1)\n",
    "    beta: Parameter vector, a numpy array of size p+1\n",
    "\n",
    "    Returns:\n",
    "    y_pred: Vector containing all binary predictions (0/1) for X_test\n",
    "    \n",
    "    \"\"\"\n",
    "    N_test = X_test.shape[0]\n",
    "    y_pred = np.zeros((N_test, 1))\n",
    "    beta0 = beta.reshape(X_test.shape[0], 1)\n",
    "    beta = beta.reshape(X_test.shape[1], 1)\n",
    "    \n",
    "    # Predicting the probabilities\n",
    "    y_log = predict_logistic(X_test, beta0, beta)\n",
    "  \n",
    "    y_pred = y_log.round().reshape(1, -1) \n",
    "  \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs the full information needed about how well the model fit and its parameters\n",
    "def model(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.1, penalty = 2.5e-3, print_loss=False):\n",
    "    # Initialising parameters with zeros\n",
    "    beta0 = initialise(X_train.shape[0])\n",
    "    beta = initialise(X_train.shape[1])\n",
    "    \n",
    "    # Gradient descent\n",
    "    parameters, grads, mean_loss_history = optimise(X_train, y_train, beta0, beta, num_iterations, learning_rate, penalty, print_loss=print_loss)\n",
    "\n",
    "    # Retrieving parameter vector beta from dictionary 'parameters'\n",
    "    beta = parameters['beta']\n",
    "    beta0 = parameters['beta0']\n",
    "    \n",
    "    # Predicting test and training set examples\n",
    "    y_pred_test = predict(X_test, beta0, beta)\n",
    "    y_pred_train = predict(X_train, beta0, beta)\n",
    "\n",
    "    # Printing train/test accuracy\n",
    "    print('Training accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "    print('Test accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "    # Saving all the information\n",
    "    d = {'mean_loss_history': mean_loss_history, 'y_pred_test': y_pred_test, 'y_pred_train': y_pred_train, 'beta0': beta0, 'beta': beta, 'learning_rate': learning_rate, 'num_iterations': num_iterations}\n",
    "  \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss after iteration 0: 0.693147\n",
      "Mean loss after iteration 100: 22332561.071913\n",
      "Mean loss after iteration 200: 86043876.816782\n",
      "Mean loss after iteration 300: 188070729.639234\n",
      "Mean loss after iteration 400: 325521353.340383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liiba\\AppData\\Local\\Temp\\ipykernel_3108\\3643047207.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  B = np.nan_to_num(np.log(1 - y_log), nan = k, posinf = k, neginf = k)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 11 into shape (42,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m model(X_log_train, y_log_train, X_log_test, y_log_test, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.5e-3\u001b[39m, print_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[78], line 15\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(X_train, y_train, X_test, y_test, num_iterations, learning_rate, penalty, print_loss)\u001b[0m\n\u001b[0;32m     12\u001b[0m beta0 \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Predicting test and training set examples\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m predict(X_test, beta0, beta)\n\u001b[0;32m     16\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m predict(X_train, beta0, beta)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Printing train/test accuracy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[77], line 14\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(X_test, beta0, beta)\u001b[0m\n\u001b[0;32m     12\u001b[0m N_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N_test, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 14\u001b[0m beta0 \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m beta \u001b[38;5;241m=\u001b[39m beta\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Predicting the probabilities\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 11 into shape (42,1)"
     ]
    }
   ],
   "source": [
    "d = model(X_log_train, y_log_train, X_log_test, y_log_test, num_iterations=500, learning_rate=0.1, penalty = 2.5e-3, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nan` values come from the algorithm ran inside the `propagation` function.\n",
    "It happens specifically when calculating `np.log(y_log)` and `np.log(1 - y_log)`.\n",
    "This is because if at least one entry, in either one of the matrices, has a value of `inf` or `nan`, the mean loss will take it into the calculation and the output will be `inf` or `nan`.\n",
    "\n",
    "I attempted to work around this however it resulted in increasing mean loss which should not be happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_loss_history\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean training loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "loss_history = np.squeeze(d['mean_loss_history'])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.ylabel('Mean training loss')\n",
    "plt.xlabel('Iterations (in hundreds)')\n",
    "plt.title('Learning curve for learning rate = ' + str(d['learning_rate']))\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_MkiUD4QB0k"
   },
   "source": [
    "<a name=\"task-222\"></a>\n",
    "\n",
    "### (2.2.2) [(index)](#index-task-222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the kernelised version of the logistic regression model from task $\\boldsymbol{2.2.1}$, the appropriate loss function will be:\n",
    "$$\n",
    "E_z(L) = -\\frac{1}{N} \\sum_{i=1}^{N} y^{(i)}logh_{\\boldsymbol{\\beta_z}, \\beta_0}(\\boldsymbol{z}^{(i)}) + (1 - y^{(i)})log(1 - h_{\\boldsymbol{\\beta_z}, \\beta_0}(\\boldsymbol{z}^{(i)})) + \\frac{1}{2}||\\boldsymbol{\\beta_z}||^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{z}^{(i)} &= \\phi(\\boldsymbol{x^{(i)}}) \\\\\n",
    "\\boldsymbol{\\beta_{z}} &= \\phi(\\boldsymbol{\\beta}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and $\\phi$ is a function such that $k(x, y) = \\phi(x)^T\\phi(y) = \\exp(-\\alpha ||x - y||_1)$. By this definition of $k$, $||\\boldsymbol{\\beta_z}||^2$ becomes $1$.\n",
    "\n",
    "Whether optimising the loss is a convex optimisation problem, we need to study the Hessian matrix of $E_z(L)$.\n",
    "\n",
    "The idea is that if $E_z(L)$ is itself not convex, then minimising it is not a convex optimisation problem. The eigenvalues of the Hessian matrix are positive if and only if the function $E_z(L)$ is concave.\n",
    "\n",
    "Before doing that let's define a few terms for simplicity:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "k_{\\boldsymbol{x^{(i)}}} &= k(\\boldsymbol{x^{(i)}}, \\boldsymbol{\\beta}) \\\\\n",
    "h_{\\boldsymbol{x^{(i)}}} &= h_{\\boldsymbol{\\beta_z}, \\beta_0}(\\boldsymbol{z}^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we can start:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left[ \\nabla_{\\boldsymbol{\\beta}}k_{\\boldsymbol{x^{(i)}}}\\right]_j &= \\frac{\\partial}{\\partial{\\beta_{j}}}k_{\\boldsymbol{x^{(i)}}} = k_{\\boldsymbol{x^{(i)}}}v_j^{(i)} \\\\\n",
    "\\left[ \\nabla_{\\boldsymbol{\\beta}} h_{\\boldsymbol{x^{(i)}}} \\right]_ j&= \\frac{\\partial}{\\partial{\\beta_{j}}}h_{\\boldsymbol{x^{(i)}}} = k_{\\boldsymbol{x^{(i)}}}\\left[h_{\\boldsymbol{x^{(i)}}}\\right]^2v_j^{(i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $v_j^{(i)} = \\alpha \\, \\mathrm{sgn}(z_j^{(i)} - \\beta_j)$.\n",
    "Moreover, $x_j^{(i)}$ and $\\beta_j$ are the $j^{th}$ component of $\\boldsymbol{z}^{(i)}$ amd $\\boldsymbol{\\beta}$\n",
    "\n",
    "Now by plugging our results and after some cancellations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial{\\beta_j}}E_z(L) = \\frac{1}{N} \\sum_{i=1}^N \\frac{(1-y^{(i)}) k_{\\boldsymbol{x^{(i)}}} h_{\\boldsymbol{x^{(i)}}}}{1 - h_{\\boldsymbol{x^{(i)}}}}v_j^{(i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By now taking the second partial derivative with respect to $\\beta_j$ and $\\beta_k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial{\\beta_j}\\partial{\\beta_k}}E_z(L) = -\\frac{1}{N}\\sum_{i=1}^{N} (1 - y^{(i)}) k_{\\boldsymbol{x^{(i)}}} h_{\\boldsymbol{x^{(i)}}} \\left[ k_{\\boldsymbol{x^{(i)}}} h_{\\boldsymbol{x^{(i)}}} - \\frac{1}{1 - h_{\\boldsymbol{x^{(i)}}}} \\right] v_j^{(i)} v_k^{(i)}\n",
    "$$\n",
    "\n",
    "Now we need to check the eigenvalues of the Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_kernel(x, y, alpha = 0.3):\n",
    "    return np.exp(-alpha*np.sum(np.abs(x-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_x = []\n",
    "h_x = []\n",
    "\n",
    "def second_deriv_E(X, y, beta, alpha = 0.3):\n",
    "    \n",
    "    N = len(y)\n",
    "    beta0 = np.zeros(N)\n",
    "    \n",
    "    for i, x in enumerate(X.T):\n",
    "        k_x.append(laplacian_kernel(x, y))\n",
    "        h_x.append(predict_logistic(x, beta0, beta))\n",
    "    \n",
    "    def sgn(x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        if x < 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    d2_E = np.zeros((N,N))\n",
    "    summing = np.zeros(N)\n",
    "    \n",
    "    for j in range(N):\n",
    "        for k in range(N):\n",
    "            for i in range(N):\n",
    "                summing[i] = alpha*(1 - y[i])*k_x[i]*h_x[i]*(k_x[i]*h_x[i]-1/(1-h_x[i]))*alpha**2*sgn(X[j,i]- beta[j])*sgn(X[k,i] - beta[k])\n",
    "            d2_E[j,k] = -np.mean(summing)\n",
    "    \n",
    "    return d2_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liiba\\AppData\\Local\\Temp\\ipykernel_3108\\3995097821.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  summing[i] = alpha*(1 - y[i])*k_x[i]*h_x[i]*(k_x[i]*h_x[i]-1/(1-h_x[i]))*alpha**2*sgn(X[j,i]- beta[j])*sgn(X[k,i] - beta[k])\n",
      "C:\\Users\\liiba\\AppData\\Local\\Temp\\ipykernel_3108\\3995097821.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  summing[i] = alpha*(1 - y[i])*k_x[i]*h_x[i]*(k_x[i]*h_x[i]-1/(1-h_x[i]))*alpha**2*sgn(X[j,i]- beta[j])*sgn(X[k,i] - beta[k])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_train)\n\u001b[0;32m      2\u001b[0m beta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;241m10\u001b[39m, N)\n\u001b[1;32m----> 3\u001b[0m second_deriv_E(X_train, y_train, beta)\n",
      "Cell \u001b[1;32mIn[82], line 27\u001b[0m, in \u001b[0;36msecond_deriv_E\u001b[1;34m(X, y, beta, alpha)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m---> 27\u001b[0m             summing[i] \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y[i])\u001b[38;5;241m*\u001b[39mk_x[i]\u001b[38;5;241m*\u001b[39mh_x[i]\u001b[38;5;241m*\u001b[39m(k_x[i]\u001b[38;5;241m*\u001b[39mh_x[i]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mh_x[i]))\u001b[38;5;241m*\u001b[39malpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39msgn(X[j,i]\u001b[38;5;241m-\u001b[39m beta[j])\u001b[38;5;241m*\u001b[39msgn(X[k,i] \u001b[38;5;241m-\u001b[39m beta[k])\n\u001b[0;32m     28\u001b[0m         d2_E[j,k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(summing)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d2_E\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "N = len(y_train)\n",
    "beta = np.random.choice(10, N)\n",
    "second_deriv_E(X_train, y_train, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzUYT08xQDV9"
   },
   "source": [
    "<a name=\"task-223\"></a>\n",
    "\n",
    "### (2.2.3) [(index)](#index-task-223)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
